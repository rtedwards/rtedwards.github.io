<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.537">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-14">
<meta name="description" content="Distributed training explained using torch distributed.">

<title>The Bear’s Toes - Distributed Training with DistributedDataParallel</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../icon/profile-circle.png" rel="icon" type="image/png">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RTR8YLS84R"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RTR8YLS84R', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  


<meta property="og:title" content="The Bear’s Toes - Distributed Training with DistributedDataParallel">
<meta property="og:description" content="Distributed training explained using torch distributed.">
<meta property="og:site_name" content="The Bear's Toes">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">The Bear’s Toes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://rtedwards.github.io/"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/robert-edwards-3b98865/"> <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img" aria-label="RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-distributeddataparallel-ddp" id="toc-what-is-distributeddataparallel-ddp" class="nav-link active" data-scroll-target="#what-is-distributeddataparallel-ddp"># What is DistributedDataParallel (DDP)?</a></li>
  <li><a href="#multi-gpu-training-with-ddp" id="toc-multi-gpu-training-with-ddp" class="nav-link" data-scroll-target="#multi-gpu-training-with-ddp"># Multi-GPU Training with DDP</a>
  <ul class="collapse">
  <li><a href="#communication-host-and-port" id="toc-communication-host-and-port" class="nav-link" data-scroll-target="#communication-host-and-port">1. Communication: host and port</a></li>
  <li><a href="#spawn-a-process-on-each-rank-gpu" id="toc-spawn-a-process-on-each-rank-gpu" class="nav-link" data-scroll-target="#spawn-a-process-on-each-rank-gpu">2. Spawn a process on each rank (GPU)</a></li>
  <li><a href="#constructing-the-process-group" id="toc-constructing-the-process-group" class="nav-link" data-scroll-target="#constructing-the-process-group">3. Constructing the process group</a></li>
  <li><a href="#constructing-the-ddp-model" id="toc-constructing-the-ddp-model" class="nav-link" data-scroll-target="#constructing-the-ddp-model">5. Constructing the DDP model</a></li>
  <li><a href="#distributing-the-data-with-distributedsampler" id="toc-distributing-the-data-with-distributedsampler" class="nav-link" data-scroll-target="#distributing-the-data-with-distributedsampler">4. Distributing the data with <code>DistributedSampler</code></a></li>
  <li><a href="#running-the-distributed-training-job" id="toc-running-the-distributed-training-job" class="nav-link" data-scroll-target="#running-the-distributed-training-job">6. Running the distributed training job</a></li>
  <li><a href="#mlflow-logging" id="toc-mlflow-logging" class="nav-link" data-scroll-target="#mlflow-logging">MLFlow logging</a></li>
  </ul></li>
  <li><a href="#gradients-losses-and-metrics" id="toc-gradients-losses-and-metrics" class="nav-link" data-scroll-target="#gradients-losses-and-metrics"># Gradients, Losses, and Metrics</a>
  <ul class="collapse">
  <li><a href="#example-torch.distributed.reduce" id="toc-example-torch.distributed.reduce" class="nav-link" data-scroll-target="#example-torch.distributed.reduce">Example: <code>torch.distributed.reduce</code></a></li>
  <li><a href="#example-torch.distributed.all_reduce" id="toc-example-torch.distributed.all_reduce" class="nav-link" data-scroll-target="#example-torch.distributed.all_reduce">Example: <code>torch.distributed.all_reduce</code></a></li>
  <li><a href="#example-torch.distributed.gather_object" id="toc-example-torch.distributed.gather_object" class="nav-link" data-scroll-target="#example-torch.distributed.gather_object">Example: <code>torch.distributed.gather_object</code></a></li>
  <li><a href="#example-torch.distributed.all_gather_object" id="toc-example-torch.distributed.all_gather_object" class="nav-link" data-scroll-target="#example-torch.distributed.all_gather_object">Example: <code>torch.distributed.all_gather_object</code></a></li>
  </ul></li>
  <li><a href="#multi-node-training" id="toc-multi-node-training" class="nav-link" data-scroll-target="#multi-node-training"># Multi-Node Training</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources"># Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Distributed Training with DistributedDataParallel</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">pytorch</div>
    <div class="quarto-category">distributed systems</div>
  </div>
  </div>

<div>
  <div class="description">
    Distributed training explained using torch distributed.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 14, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="what-is-distributeddataparallel-ddp" class="level2">
<h2 class="anchored" data-anchor-id="what-is-distributeddataparallel-ddp"># What is DistributedDataParallel (DDP)?</h2>
<p><code>DistributedDataParallel</code> is a way to parallelize training across multiple GPUs or nodes. It is an extension of <code>DataParallel</code> that provides more flexibility and scalability. <code>DataParallel</code> (DP) is an older approach to data parallelism. DP is trivially simple (with just one extra line of code) but it is less performant. DDP improves upon the architecture in a few ways:</p>
<table class="table">
<colgroup>
<col style="width: 63%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th><code>DataParallel</code></th>
<th><code>DistributedDataParallel</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simpler to use</td>
<td>More involved changes to use</td>
</tr>
<tr class="even">
<td>More overhead; model is replicated and destroyed at each forward pass</td>
<td>Model is replicated only once at the start</td>
</tr>
<tr class="odd">
<td>Only supports single-node parallelism</td>
<td>Supports single-node and multi-node parallelism</td>
</tr>
<tr class="even">
<td>Slower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention</td>
<td>Faster (no GIL contention) because it uses multiprocessing</td>
</tr>
</tbody>
</table>
</section>
<section id="multi-gpu-training-with-ddp" class="level2">
<h2 class="anchored" data-anchor-id="multi-gpu-training-with-ddp"># Multi-GPU Training with DDP</h2>
<p>DDP uses multiprocessing to copy the model to each GPU (<code>rank</code>). This allows the model (and code) to only be copied to each process once at the start of the script. Multiprocessing pickles Python objects to serialize across processes. This means <em>all</em> objects must be <a href="https://docs.python.org/3/library/pickle.html">pickleable</a>.</p>
<div class="callout callout-style-default callout-important callout-titled" title="All objects sent to each process must be pickleable">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
All objects sent to each process must be pickleable
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled">What can be pickled and unpickled?</a></p>
<p><strong>Some objects that can’t be pickled:</strong></p>
<ul>
<li>Generators</li>
<li>Database connections</li>
<li>Sockets</li>
<li>File descriptors</li>
<li>Lambdas</li>
</ul>
</div>
</div>
<p>The basic outline of DDP training is:</p>
<ol type="1">
<li>Setup the communications by setting the host and port</li>
<li>Spawn a training process for each GPU with <code>torch.multiprocessing.spawn</code></li>
<li>Initialize the process group using <code>init_process_group</code>:
<ul>
<li>GPU - <code>"nccl"</code></li>
<li>CPU - <code>"gloo"</code></li>
</ul></li>
<li>Wrap the model with <code>DistributedDataParallel</code></li>
<li>Create a <code>DistributedSampler</code> and <code>DataLoader</code> for the dataset</li>
<li>Train the model and update sampler with the epoch</li>
<li>Destroy the process group using <code>destroy_process_group</code></li>
</ol>
<div id="multi-gpu" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.distributed <span class="im">import</span> DistributedSampler</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group, destroy_process_group</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    rank: <span class="bu">int</span>, <span class="co"># rank is the GPU number</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    world_size: <span class="bu">int</span>, <span class="co"># world_size is the number of processes, typically set to the number of GPUs</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    train_path: <span class="bu">str</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    random_state: <span class="bu">int</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    lr: <span class="bu">float</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    epochs: <span class="bu">int</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    num_workers: <span class="bu">int</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    batch_size: <span class="bu">int</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... Other setup</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize the process group for distributed training</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we need to divide the workers and batch across the different processes used in distributed training</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    num_workers_per_proc <span class="op">=</span> num_workers <span class="op">//</span> world_size <span class="co"># avoids CPU contentionn</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    batch_size_per_proc <span class="op">=</span> batch_size <span class="op">//</span> world_size   <span class="co"># avoids OOM</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># DistributedSampler ensures that training data is chunked across GPUs without overlapping samples</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    train_sampler <span class="op">=</span> DistributedSampler(train_dataset)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    val_sampler <span class="op">=</span> DistributedSampler(</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        val_dataset,</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle the validation dataset</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        drop_last<span class="op">=</span><span class="va">True</span>, <span class="co"># DistributedSampler will append additional samples to fill an incomplete batch.  We don't want that for the validation dataset.</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        train_dataset,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle if using DistributedSampler as that's done within the sampler</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        sampler<span class="op">=</span>train_sampler,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    val_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        val_dataset,</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        sampler<span class="op">=</span>val_sampler,</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up the NN model as normal and then wrap with DDP</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    model.to(rank)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> nn.parallel.DistributedDataParallel(model, device_ids<span class="op">=</span>[rank])</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># need to call `set_epoch()` at the beginning of each epoch before creating the</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `DataLoader` iterator to make shuffling work properly across multiple epochs</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        train_sampler.set_epoch(epch)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... training loop</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... diagnostics</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cleanly shutdown distributed processes</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> os</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span> <span class="co"># for single node on local compute</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span> <span class="co"># any free port</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    world_size <span class="op">=</span> torch.cuda.device_count()  <span class="co"># number of GPUs</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... CLI args parsing</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># spawn multiple processes equal to world_size first argument passed in will be the rank</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    torch.multiprocessing.spawn(</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>        main,</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>(</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>            world_size,</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>            args.train_path,</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>            args.random_state,</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>            args.lr,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>            args.epochs,</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            args.num_workers,</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>            args.batch_size,</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        nprocs<span class="op">=</span>world_size,  <span class="co"># this is used to set the `rank` parameter.  It is passed as the first argument</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="communication-host-and-port" class="level3">
<h3 class="anchored" data-anchor-id="communication-host-and-port">1. Communication: host and port</h3>
<p>Setting up distributed training on a single (local) node is as simple as setting the host and port as below. To setup multi-node see <a href="https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html">torchrun</a>.</p>
<div id="host-and-port" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span> <span class="co"># for single node on local compute</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span> <span class="co"># any free port</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="spawn-a-process-on-each-rank-gpu" class="level3">
<h3 class="anchored" data-anchor-id="spawn-a-process-on-each-rank-gpu">2. Spawn a process on each rank (GPU)</h3>
<p>Since <code>torch.multiprocessing</code> follows the same API as <code>multiprocessing</code>. To spawn a new process we pass the function to run, the arguments as a tuple, and specify the number of processes (usually the number of GPUs).</p>
<div id="spawn-process-on-each-rank" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># spawn multiple processes equal to world_size first argument passed in will be the rank</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>torch.multiprocessing.spawn(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    main,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>(</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        world_size,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        args.train_path,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        args.random_state,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        args.lr,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        args.epochs,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        args.num_workers,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        args.batch_size,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    nprocs<span class="op">=</span>world_size,  <span class="co"># this is used to set the `rank` parameter.  It is passed as the first argument</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="constructing-the-process-group" class="level3">
<h3 class="anchored" data-anchor-id="constructing-the-process-group">3. Constructing the process group</h3>
<ul>
<li>First, before initializing the group process, call <a href="https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html?highlight=set_device#torch.cuda.set_device">set_device</a>, which sets the default GPU for each process. This is important to prevent hangs or excessive memory utilization on GPU:0</li>
<li>The process group can be initialized by TCP (default) or from a shared file-system. Read more on <a href="https://pytorch.org/docs/stable/distributed.html#tcp-initialization">process group initialization</a>.</li>
<li><a href="https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group">init_process_group</a> initializes the distributed process group.</li>
<li>Read more about <a href="https://pytorch.org/docs/stable/distributed.html#which-backend-to-use">choosing a DDP backend</a>.</li>
</ul>
<div id="initializing-process-group" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the process group for distributed training</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>init_process_group(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    rank<span class="op">=</span>rank,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    world_size<span class="op">=</span>world_size,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="constructing-the-ddp-model" class="level3">
<h3 class="anchored" data-anchor-id="constructing-the-ddp-model">5. Constructing the DDP model</h3>
<ul>
<li><code>device_ids</code> - 1) For single-device modules, device_ids can contain exactly one device id, which represents the only CUDA device where the input module corresponding to this process resides. Alternatively, device_ids can also be None. 2) For multi-device modules and CPU modules, device_ids must be None. (From the <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DDP docs</a>)</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model.to(rank)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.parallel.DistributedDataParallel(model, device_ids<span class="op">=</span>[rank])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="distributing-the-data-with-distributedsampler" class="level3">
<h3 class="anchored" data-anchor-id="distributing-the-data-with-distributedsampler">4. Distributing the data with <code>DistributedSampler</code></h3>
<section id="dividing-the-workload" class="level4">
<h4 class="anchored" data-anchor-id="dividing-the-workload">Dividing the workload</h4>
<ul>
<li><a href="https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler"><code>DistributedSampler</code></a> chunks the input data across all distributed processes, without overlap. If we have 4 GPUs then each process will only load 1/4 of the training dataset.</li>
<li>The <code>batch_size</code> needs to be divided among the processes (GPUs). Each process will receive an input batch of <code>batch_size_per_proc</code>; the effective batch size is <code>batch_size_per_proc</code> * <code>world_size</code>, if the <code>batch_size</code> is 64 and <code>world_size</code> is 4 GPUs, then the effective batch size is still 64 in total.</li>
<li>The <code>num_workers</code> also needs to be divided among the processes (GPUs). Each proces will receive <code>num_workers_per_proc</code>.</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled" title="`batch_size` and OOM">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<code>batch_size</code> and OOM
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the batch_size isn’t divided among the processes then then each process gets a full batch and the effective batch size is now x<code>world_size</code> larger and we are likely to run out of CPU or GPU memory if not careful.</p>
</div>
</div>
<div id="distributing-workload-across-workers" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to divide the workers and batch across the different processes used in distributed training</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>num_workers_per_proc <span class="op">=</span> num_workers <span class="op">//</span> world_size <span class="co"># avoids CPU contention</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>batch_size_per_proc <span class="op">=</span> batch_size <span class="op">//</span> world_size   <span class="co"># avoids OOM</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="setting-up-the-distributedsampler" class="level4">
<h4 class="anchored" data-anchor-id="setting-up-the-distributedsampler">Setting Up the DistributedSampler</h4>
<ul>
<li><code>shuffle</code> - by default, the <code>DistributedSampler</code> will shuffle the dataset. We don’t want to shuffle the validation dataset.</li>
<li><code>drop_last</code> - by default, the <code>DistributedSampler</code> will append additional samples to fill an incomplete batch (e.g.&nbsp;there’s 100 training samples with <code>batch_size=64</code> there would be one batch of 36 samples). We don’t want to repeat samples for the validation dataset as that would change the metrics.</li>
<li><code>pin_memory</code> - For large datasets that are loaded into the CPU in the <code>Dataset</code>, pinning the memory can speed up the host to device transfer (see this <a href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned_host_memory">NVIDIA blog</a> for more details).</li>
</ul>
<div id="distributed-sampler" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DistributedSampler ensures that training data is chunked across GPUs without overlapping samples</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>train_sampler <span class="op">=</span> DistributedSampler(train_dataset)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>val_sampler <span class="op">=</span> DistributedSampler(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    val_dataset,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle the validation dataset</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    drop_last<span class="op">=</span><span class="va">True</span>, <span class="co"># DistributedSampler will append additional samples to fill an incomplete batch.  We don't want that for the validation dataset.</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    train_dataset,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle if using DistributedSampler as that's done within the sampler</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>train_sampler,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>val_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    val_dataset,</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>val_sampler,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="shuffling-across-epochs" class="level4">
<h4 class="anchored" data-anchor-id="shuffling-across-epochs">7. Shuffling across epochs</h4>
<ul>
<li>Calling the <code>set_epoch()</code> method on the <code>DistributedSampler</code> at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch.</li>
</ul>
<div id="shuffling-across-epochs" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ... Neural Network setup</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># need to call `set_epoch()` at the beginning of each epoch before creating the</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `DataLoader` iterator to make shuffling work properly across multiple epochs</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    train_sampler.set_epoch(epch)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... training loop</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="running-the-distributed-training-job" class="level3">
<h3 class="anchored" data-anchor-id="running-the-distributed-training-job">6. Running the distributed training job</h3>
<ul>
<li><code>rank</code> is auto-allocated by DDP when calling <a href="https://pytorch.org/docs/stable/multiprocessing.html#spawning-subprocesses"><code>torch.multiprocessing.spawn</code></a>.</li>
<li><code>world_size</code> is the number of processes across the training job. For GPU training, this corresponds to the number of GPUs in use, and each process works on a dedicated GPU.</li>
<li>Both <code>rank</code> and <code>world_size</code> are new parameters to <code>main()</code>. Because of how spawning processes works, <code>rank</code> <em>needs</em> to be the first parameter to the calling function, <code>main(rank, ...)</code>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="PyTorch Multiprocessing">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
PyTorch Multiprocessing
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://pytorch.org/docs/stable/multiprocessing.html">PyTorch’s <code>torch.multiprocessing</code> package</a> is a wrapper around the native <code>multiprocessing</code> module and the API is 100% compatible.</p>
</div>
</div>
<div id="mean-and-variance" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... CLI args parsing</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>world_size <span class="op">=</span> torch.cuda.device_count()  <span class="co"># number of GPUs</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>torch.multiprocessing.spawn(</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    main,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>(</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        world_size,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        args.train_path,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        args.random_state,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        args.lr,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        args.epochs,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        args.num_workers,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        args.batch_size,</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    nprocs<span class="op">=</span>world_size,  <span class="co"># this is used to set the `rank` parameter.  It is passed as the first argument</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="mlflow-logging" class="level3">
<h3 class="anchored" data-anchor-id="mlflow-logging">MLFlow logging</h3>
<p>Since there are now multiple processes runnning the same code, the same logging will happen on each process. MLFlow doesn’t know how to distinguish that there are different processes logging the same metric. We can guard against this by only logging on the main process (GPU0):</p>
<div id="mlflow-and-torch.distributed" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    mlflow.log_metrics(</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"train loss"</span>: train_loss,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"train accuracy"</span>: train_accuracy,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"val loss"</span>: val_loss,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"val accuracy"</span>: val_accuracy,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        step<span class="op">=</span>epoch,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="gradients-losses-and-metrics" class="level2">
<h2 class="anchored" data-anchor-id="gradients-losses-and-metrics"># Gradients, Losses, and Metrics</h2>
<p>Under the hood, DDP synchronizes and gathers the gradients across all processes. However, any other ad-hoc value calculated in your code is not; e.g.&nbsp;losses and metrics.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Gradients are synchronized">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradients are synchronized
</div>
</div>
<div class="callout-body-container callout-body">
<p>Model gradients are synchronized across processes during the backward pass. This means that the model in each process is the same! <a href="https://pytorch.org/docs/master/notes/ddp.html#internal-design">See DDP: Internal Design</a>.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled" title="Losses are not synchronized">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Losses are not synchronized
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even though the model is the same in each process, the loss is calculated on only the portion of the batch that each process sees. The losses don’t need to be synchronized for training but we may want to synchronize the losses for logging or definitelty when calculating metrics on the hold-out (validation) dataset.</p>
<p>We could avoid this by <em>not</em> using a <code>DistributedSampler</code> for the validation set, but then only 1 process would be used to calculate the loss for the whole validation set each epoch, which will be <em>slow</em>.</p>
</div>
</div>
<p>So if each process is calculating and accumulating losses and metrics separately, how do we log those and report as if there were a single process? Well, those values will need to be gathered and then accumulated. Say we have 4 processes, one for each GPU, and each is processing 1/4 of the training dataset. We want to report the loss for each epoch. If we log the loss in each process, we will have 4 different losses. We can gather and combine them in a few ways. Since the loss is just a number value we can use <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce"><code>torch.distributed.reduce</code></a> or <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce"><code>torch.distributed.all_reduce</code></a>:</p>
<section id="example-torch.distributed.reduce" class="level3">
<h3 class="anchored" data-anchor-id="example-torch.distributed.reduce">Example: <code>torch.distributed.reduce</code></h3>
<p>In this example, we gather and combine using summation with the <code>dist.ReduceOp.SUM</code>, all the <code>loss_tensor</code>s into the process <code>0</code> tensor (<code>dst=0</code>). Each tensor in each process must be the same shape. Since we are assigning the values in each processes’s <code>loss_tensor</code> to it’s rank, we expect the final gathered values to be <code>0 + 1 + 2 + 3 = 6</code> in the main process <code>loss_tensor</code>.</p>
<div id="example-torch.distributed.reduce" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reduce_tensor(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    loss_tensor <span class="op">=</span> torch.tensor([rank, rank]).cuda()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    dist.<span class="bu">reduce</span>(loss_tensor, op<span class="op">=</span>dist.ReduceOp.SUM, dst<span class="op">=</span><span class="dv">0</span>, async_op<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    mp.spawn(reduce_tensor, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre class="{text}"><code>tensor([1, 1], device='cuda:1')
tensor([3, 3], device='cuda:3')
tensor([2, 2], device='cuda:2')
tensor([0, 0], device='cuda:0')

tensor([6, 6], device='cuda:0')
tensor([1, 1], device='cuda:1')
tensor([2, 2], device='cuda:2')
tensor([3, 3], device='cuda:3')</code></pre>
</section>
<section id="example-torch.distributed.all_reduce" class="level3">
<h3 class="anchored" data-anchor-id="example-torch.distributed.all_reduce">Example: <code>torch.distributed.all_reduce</code></h3>
<p>In this example, we gather and combine using summation with the <code>dist.ReduceOp.SUM</code>, all the <code>loss_tensor</code>s into all the processes. Each tensor in each process must be the same shape. Since we are assigning the values in each processes’s <code>loss_tensor</code> to it’s rank, we expect the final gathered values to be <code>0 + 1 + 2 + 3 = 6</code> in the all the processes’s <code>loss_tensor</code>.</p>
<div id="example-torch.distributed.all_reduce" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_reduce_tensor(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    loss_tensor <span class="op">=</span> torch.tensor([rank, rank]).cuda()</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    dist.all_reduce(loss_tensor, op<span class="op">=</span>dist.ReduceOp.SUM, dst<span class="op">=</span><span class="dv">0</span>, async_op<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    mp.spawn(all_reduce_tensor, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre class="{text}"><code>tensor([2, 2], device='cuda:2')
tensor([3, 3], device='cuda:3')
tensor([1, 1], device='cuda:1')
tensor([0, 0], device='cuda:0')

tensor([6, 6], device='cuda:2')
tensor([6, 6], device='cuda:3')
tensor([6, 6], device='cuda:1')
tensor([6, 6], device='cuda:0')</code></pre>
</section>
<section id="example-torch.distributed.gather_object" class="level3">
<h3 class="anchored" data-anchor-id="example-torch.distributed.gather_object">Example: <code>torch.distributed.gather_object</code></h3>
<p>Syncing across processing is simple enough for tensors, but if we have a number of values to gather (say a bunch of metrics for example) it would be easier to only need to gather once and store those values in an appropriate data structure. Most of the distributed gathering function only work on tensors, but we can use <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather_object"><code>torch.distributed.gather_object</code></a> and / or <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object"><code>torch.distributed.all_gather_object</code></a> to pass pickleable Python objects between ranks.</p>
<p>In this example, we want to track the losses and number of samples in a <code>Counter</code> so that we can combine and calculate the mean loss after gathering. We gather each loss counter to rank 0. Each counter is placed into the <code>gather_list</code>, which must have all elements set to <code>None</code> initially. When calling, <code>dist.gather_object</code>, the <code>gather_list</code> must only exist in the rank being gathered to (<code>dst=0</code> or rank 0 in this case). Then we use <code>functools.reduce</code> to sum all the Counters gathered in the <code>gather_list</code>.</p>
<div id="example-torch.distributed.gather_object" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> operator</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gather_object(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> Counter(</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"loss"</span>: <span class="fl">0.01</span>, <span class="st">"num_samples"</span>: rank}</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    gather_list <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    dist.gather_object(losses, gather_list <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">None</span>, dst<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> functools.<span class="bu">reduce</span>(operator.add, gather_list)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    mp.spawn(gather_object, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre class="{text}"><code>Counter({'num_samples': 1, 'loss': 0.01})
Counter({'num_samples': 2, 'loss': 0.01})
Counter({'num_samples': 4, 'loss': 0.01})
Counter({'num_samples': 3, 'loss': 0.01})

Counter({'num_samples': 10, 'loss': 0.04})
Counter({'num_samples': 4, 'loss': 0.01})
Counter({'num_samples': 3, 'loss': 0.01})
Counter({'num_samples': 2, 'loss': 0.01})</code></pre>
</section>
<section id="example-torch.distributed.all_gather_object" class="level3">
<h3 class="anchored" data-anchor-id="example-torch.distributed.all_gather_object">Example: <code>torch.distributed.all_gather_object</code></h3>
<p>In this example, we want to track the losses and number of samples in a <code>Counter</code> so that we can combine and calculate the mean loss after gathering. We gather each loss counter to each rank. Each counter is placed into the <code>gather_list</code>, which must have all elements set to <code>None</code> initially. Then we use <code>functools.reduce</code> to sum all the Counters gathered in the <code>gather_list</code>.</p>
<div id="example-torch.distributed.all_gather_object" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> operator</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_gather_object(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> Counter(</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"loss"</span>: <span class="fl">0.01</span>, <span class="st">"num_samples"</span>: rank}</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    gather_list <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    dist.all_gather_object(gather_list, losses)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> functools.<span class="bu">reduce</span>(operator.add, gather_list)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    mp.spawn(all_gather_object, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre class="{text}"><code>Counter({'num_samples': 1, 'loss': 0.01})
Counter({'num_samples': 4, 'loss': 0.01})
Counter({'num_samples': 2, 'loss': 0.01})
Counter({'num_samples': 3, 'loss': 0.01})

Counter({'num_samples': 10, 'loss': 0.04})
Counter({'num_samples': 10, 'loss': 0.04})
Counter({'num_samples': 10, 'loss': 0.04})
Counter({'num_samples': 10, 'loss': 0.04})</code></pre>
</section>
</section>
<section id="multi-node-training" class="level2">
<h2 class="anchored" data-anchor-id="multi-node-training"># Multi-Node Training</h2>
<p>Now that we’ve setup DDP, we have the option of running on multiple nodes. There are a few extra bits that need to be changed before we run <a href="https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html">multinode training</a>:</p>
<ul>
<li>change a few lines to enable <a href="https://pytorch.org/docs/stable/elastic/run.html"><code>torchrun</code></a></li>
<li>install the project as a python package for <code>torchrun</code> to work properly</li>
<li>add <a href="https://pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html">fault tolerance</a></li>
</ul>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources"># Resources</h2>
<ol type="1">
<li><a href="https://pytorch.org/docs/stable/notes/ddp.html"><code>DistributedDataParallel</code></a> - Documentation for DDP.</li>
<li><a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a> - Good starting point to understand DDP and writing a training script using DDP for single-node and multi-node.</li>
<li><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html">Writing Distributed Applications with Pytorch</a> - In-depth article about writing distributed applications in PyTorch and how communication works under the hood.</li>
<li><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler"><code>DistributedSampler</code></a> - Used in conjunction with a DataLoader, the DistributedSampler enables each process to only load the data it processes, rather than all the data to be processed.</li>
<li><a href="https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html">DistributedDataParallel training in PyTorch</a> - Explaination of how DDP works and how to use it.</li>
<li><a href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-distributed-gpu?view=azureml-api-2">(AML) Distributed GPU training guide (SDK v2)</a></li>
<li><a href="https://ochzhen.com/blog/pytorch-distributed-data-parallel-azure-ml">PyTorch DistributedDataParallel Example In Azure ML - Multi-Node Multi-GPU Distributed Training</a> - Example of DDP for AML.</li>
<li><a href="https://arxiv.org/pdf/2006.15704">PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a> - Paper on how Facebook designed DDP to be faster for distributed training.</li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb19" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Distributed Training with DistributedDataParallel"</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-04-14"</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [deep learning, pytorch, distributed systems]</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Distributed training explained using torch distributed."</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="an">reading-time:</span><span class="co"> true</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="an">reference-location:</span><span class="co"> document</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="an">citation-location:</span><span class="co"> document</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># bibliography: references.bib</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="an">citations-hover:</span><span class="co"> true</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: show</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">    code-summary: ""</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## # What is DistributedDataParallel (DDP)?</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="in">`DistributedDataParallel`</span> is a way to parallelize training across multiple GPUs or nodes. It is an extension of <span class="in">`DataParallel`</span> that provides more flexibility and scalability. <span class="in">`DataParallel`</span> (DP) is an older approach to data parallelism. DP is trivially simple (with just one extra line of code) but it is less performant. DDP improves upon the architecture in a few ways:</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>|<span class="in">`DataParallel`</span>                                                                                          | <span class="in">`DistributedDataParallel`</span>                                  |</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------|</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>| Simpler to use                                                                                         | More involved changes to use                               |</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>| More overhead; model is replicated and destroyed at each forward pass                                  | Model is replicated only once at the start                 |</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>| Only supports single-node parallelism                                                                  | Supports single-node and multi-node parallelism            |</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>| Slower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention | Faster (no GIL contention) because it uses multiprocessing |</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## # Multi-GPU Training with DDP</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>DDP uses multiprocessing to copy the model to each GPU (<span class="in">`rank`</span>).  This allows the model (and code) to only be copied to each process once at the start of the script.  Multiprocessing pickles Python objects to serialize across processes.  This means _all_ objects must be <span class="co">[</span><span class="ot">pickleable</span><span class="co">](https://docs.python.org/3/library/pickle.html)</span>.</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>::: {.callout-important title="All objects sent to each process must be pickleable"}</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">What can be pickled and unpickled?</span><span class="co">](https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled)</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>**Some objects that can't be pickled:**</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generators</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Database connections</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sockets</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>File descriptors</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lambdas</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>The basic outline of DDP training is:</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Setup the communications by setting the host and port</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Spawn a training process for each GPU with <span class="in">`torch.multiprocessing.spawn`</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Initialize the process group using <span class="in">`init_process_group`</span>:</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>GPU - <span class="in">`"nccl"`</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>CPU - <span class="in">`"gloo"`</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Wrap the model with <span class="in">`DistributedDataParallel`</span></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Create a <span class="in">`DistributedSampler`</span> and <span class="in">`DataLoader`</span> for the dataset</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Train the model and update sampler with the epoch</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Destroy the process group using <span class="in">`destroy_process_group`</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: multi-gpu</span></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.distributed <span class="im">import</span> DistributedSampler</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group, destroy_process_group</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(</span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>    rank: <span class="bu">int</span>, <span class="co"># rank is the GPU number</span></span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>    world_size: <span class="bu">int</span>, <span class="co"># world_size is the number of processes, typically set to the number of GPUs</span></span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>    train_path: <span class="bu">str</span>,</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a>    random_state: <span class="bu">int</span>,</span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a>    lr: <span class="bu">float</span>,</span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>    epochs: <span class="bu">int</span>,</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a>    num_workers: <span class="bu">int</span>,</span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>    batch_size: <span class="bu">int</span>,</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... Other setup</span></span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize the process group for distributed training</span></span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we need to divide the workers and batch across the different processes used in distributed training</span></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a>    num_workers_per_proc <span class="op">=</span> num_workers <span class="op">//</span> world_size <span class="co"># avoids CPU contentionn</span></span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a>    batch_size_per_proc <span class="op">=</span> batch_size <span class="op">//</span> world_size   <span class="co"># avoids OOM</span></span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a>    <span class="co"># DistributedSampler ensures that training data is chunked across GPUs without overlapping samples</span></span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a>    train_sampler <span class="op">=</span> DistributedSampler(train_dataset)</span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a>    val_sampler <span class="op">=</span> DistributedSampler(</span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a>        val_dataset,</span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle the validation dataset</span></span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a>        drop_last<span class="op">=</span><span class="va">True</span>, <span class="co"># DistributedSampler will append additional samples to fill an incomplete batch.  We don't want that for the validation dataset.</span></span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a>    train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a>        train_dataset,</span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle if using DistributedSampler as that's done within the sampler</span></span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a>        sampler<span class="op">=</span>train_sampler,</span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a>        num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a>        pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a>        collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a>    val_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a>        val_dataset,</span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a>        sampler<span class="op">=</span>val_sampler,</span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a>        num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a>        pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a>        collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up the NN model as normal and then wrap with DDP</span></span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a>    model.to(rank)</span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> nn.parallel.DistributedDataParallel(model, device_ids<span class="op">=</span>[rank])</span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># need to call `set_epoch()` at the beginning of each epoch before creating the</span></span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `DataLoader` iterator to make shuffling work properly across multiple epochs</span></span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a>        train_sampler.set_epoch(epch)</span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... training loop</span></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... diagnostics</span></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cleanly shutdown distributed processes</span></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> os</span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span> <span class="co"># for single node on local compute</span></span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span> <span class="co"># any free port</span></span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a>    world_size <span class="op">=</span> torch.cuda.device_count()  <span class="co"># number of GPUs</span></span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... CLI args parsing</span></span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a>    <span class="co"># spawn multiple processes equal to world_size first argument passed in will be the rank</span></span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a>    torch.multiprocessing.spawn(</span>
<span id="cb19-149"><a href="#cb19-149" aria-hidden="true" tabindex="-1"></a>        main,</span>
<span id="cb19-150"><a href="#cb19-150" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>(</span>
<span id="cb19-151"><a href="#cb19-151" aria-hidden="true" tabindex="-1"></a>            world_size,</span>
<span id="cb19-152"><a href="#cb19-152" aria-hidden="true" tabindex="-1"></a>            args.train_path,</span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a>            args.random_state,</span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a>            args.lr,</span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a>            args.epochs,</span>
<span id="cb19-156"><a href="#cb19-156" aria-hidden="true" tabindex="-1"></a>            args.num_workers,</span>
<span id="cb19-157"><a href="#cb19-157" aria-hidden="true" tabindex="-1"></a>            args.batch_size,</span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a>        nprocs<span class="op">=</span>world_size,  <span class="co"># this is used to set the `rank` parameter.  It is passed as the first argument</span></span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Communication: host and port</span></span>
<span id="cb19-164"><a href="#cb19-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-165"><a href="#cb19-165" aria-hidden="true" tabindex="-1"></a>Setting up distributed training on a single (local) node is as simple as setting the host and port as below.  To setup multi-node see <span class="co">[</span><span class="ot">torchrun</span><span class="co">](https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html)</span>.</span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: host and port</span></span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span> <span class="co"># for single node on local compute</span></span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span> <span class="co"># any free port</span></span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Spawn a process on each rank (GPU)</span></span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a>Since <span class="in">`torch.multiprocessing`</span> follows the same API as <span class="in">`multiprocessing`</span>.  To spawn a new process we pass the function to run, the arguments as a tuple, and specify the number of processes (usually the number of GPUs).</span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-183"><a href="#cb19-183" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-184"><a href="#cb19-184" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: spawn process on each rank</span></span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-189"><a href="#cb19-189" aria-hidden="true" tabindex="-1"></a><span class="co"># spawn multiple processes equal to world_size first argument passed in will be the rank</span></span>
<span id="cb19-190"><a href="#cb19-190" aria-hidden="true" tabindex="-1"></a>torch.multiprocessing.spawn(</span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a>    main,</span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>(</span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a>        world_size,</span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a>        args.train_path,</span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a>        args.random_state,</span>
<span id="cb19-196"><a href="#cb19-196" aria-hidden="true" tabindex="-1"></a>        args.lr,</span>
<span id="cb19-197"><a href="#cb19-197" aria-hidden="true" tabindex="-1"></a>        args.epochs,</span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a>        args.num_workers,</span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a>        args.batch_size,</span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a>    nprocs<span class="op">=</span>world_size,  <span class="co"># this is used to set the `rank` parameter.  It is passed as the first argument</span></span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Constructing the process group</span></span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First, before initializing the group process, call <span class="co">[</span><span class="ot">set_device</span><span class="co">](https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html?highlight=set_device#torch.cuda.set_device)</span>, which sets the default GPU for each process. This is important to prevent hangs or excessive memory utilization on GPU:0</span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The process group can be initialized by TCP (default) or from a shared file-system. Read more on <span class="co">[</span><span class="ot">process group initialization</span><span class="co">](https://pytorch.org/docs/stable/distributed.html#tcp-initialization)</span>.</span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">init_process_group</span><span class="co">](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)</span> initializes the distributed process group.</span>
<span id="cb19-210"><a href="#cb19-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Read more about <span class="co">[</span><span class="ot">choosing a DDP backend</span><span class="co">](https://pytorch.org/docs/stable/distributed.html#which-backend-to-use)</span>.</span>
<span id="cb19-211"><a href="#cb19-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-214"><a href="#cb19-214" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: initializing process group</span></span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the process group for distributed training</span></span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a>init_process_group(</span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a>    backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb19-223"><a href="#cb19-223" aria-hidden="true" tabindex="-1"></a>    rank<span class="op">=</span>rank,</span>
<span id="cb19-224"><a href="#cb19-224" aria-hidden="true" tabindex="-1"></a>    world_size<span class="op">=</span>world_size,</span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5. Constructing the DDP model</span></span>
<span id="cb19-229"><a href="#cb19-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-230"><a href="#cb19-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`device_ids`</span> - 1) For single-device modules, device_ids can contain exactly one device id, which represents the only CUDA device where the input module corresponding to this process resides. Alternatively, device_ids can also be None. 2) For multi-device modules and CPU modules, device_ids must be None.  (From the <span class="co">[</span><span class="ot">DDP docs</span><span class="co">](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)</span>)</span>
<span id="cb19-231"><a href="#cb19-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-232"><a href="#cb19-232" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb19-233"><a href="#cb19-233" aria-hidden="true" tabindex="-1"></a>model.to(rank)</span>
<span id="cb19-234"><a href="#cb19-234" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.parallel.DistributedDataParallel(model, device_ids<span class="op">=</span>[rank])</span>
<span id="cb19-235"><a href="#cb19-235" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-236"><a href="#cb19-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-237"><a href="#cb19-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. Distributing the data with `DistributedSampler`</span></span>
<span id="cb19-238"><a href="#cb19-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-239"><a href="#cb19-239" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Dividing the workload</span></span>
<span id="cb19-240"><a href="#cb19-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-241"><a href="#cb19-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">`DistributedSampler`</span><span class="co">](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler)</span> chunks the input data across all distributed processes, without overlap.  If we have 4 GPUs then each process will only load 1/4 of the training dataset.</span>
<span id="cb19-242"><a href="#cb19-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The <span class="in">`batch_size`</span> needs to be divided among the processes (GPUs).  Each process will receive an input batch of <span class="in">`batch_size_per_proc`</span>; the effective batch size is <span class="in">`batch_size_per_proc`</span> * <span class="in">`world_size`</span>, if the <span class="in">`batch_size`</span> is 64 and <span class="in">`world_size`</span> is 4 GPUs, then the effective batch size is still 64 in total.</span>
<span id="cb19-243"><a href="#cb19-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The <span class="in">`num_workers`</span> also needs to be divided among the processes (GPUs).  Each proces will receive <span class="in">`num_workers_per_proc`</span>.</span>
<span id="cb19-244"><a href="#cb19-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-245"><a href="#cb19-245" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution title="<span class="in">`batch_size`</span> and OOM"}</span>
<span id="cb19-246"><a href="#cb19-246" aria-hidden="true" tabindex="-1"></a>If the batch_size isn't divided among the processes then then each process gets a full batch and the effective batch size is now x<span class="in">`world_size`</span> larger and we are likely to run out of CPU or GPU memory if not careful.</span>
<span id="cb19-247"><a href="#cb19-247" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-248"><a href="#cb19-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-251"><a href="#cb19-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-252"><a href="#cb19-252" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: distributing workload across workers</span></span>
<span id="cb19-253"><a href="#cb19-253" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-254"><a href="#cb19-254" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-255"><a href="#cb19-255" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to divide the workers and batch across the different processes used in distributed training</span></span>
<span id="cb19-256"><a href="#cb19-256" aria-hidden="true" tabindex="-1"></a>num_workers_per_proc <span class="op">=</span> num_workers <span class="op">//</span> world_size <span class="co"># avoids CPU contention</span></span>
<span id="cb19-257"><a href="#cb19-257" aria-hidden="true" tabindex="-1"></a>batch_size_per_proc <span class="op">=</span> batch_size <span class="op">//</span> world_size   <span class="co"># avoids OOM</span></span>
<span id="cb19-258"><a href="#cb19-258" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-259"><a href="#cb19-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-260"><a href="#cb19-260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Setting Up the DistributedSampler</span></span>
<span id="cb19-261"><a href="#cb19-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-262"><a href="#cb19-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`shuffle`</span> - by default, the <span class="in">`DistributedSampler`</span> will shuffle the dataset.  We don't want to shuffle the validation dataset.</span>
<span id="cb19-263"><a href="#cb19-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`drop_last`</span> - by default, the <span class="in">`DistributedSampler`</span> will append additional samples to fill an incomplete batch (e.g. there's 100 training samples with <span class="in">`batch_size=64`</span> there would be one batch of 36 samples).  We don't want to repeat samples for the validation dataset as that would change the metrics.</span>
<span id="cb19-264"><a href="#cb19-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`pin_memory`</span> - For large datasets that are loaded into the CPU in the <span class="in">`Dataset`</span>, pinning the memory can speed up the host to device transfer (see this <span class="co">[</span><span class="ot">NVIDIA blog</span><span class="co">](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned_host_memory)</span> for more details).</span>
<span id="cb19-265"><a href="#cb19-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-268"><a href="#cb19-268" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-269"><a href="#cb19-269" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: distributed sampler</span></span>
<span id="cb19-270"><a href="#cb19-270" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-271"><a href="#cb19-271" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-272"><a href="#cb19-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-273"><a href="#cb19-273" aria-hidden="true" tabindex="-1"></a><span class="co"># DistributedSampler ensures that training data is chunked across GPUs without overlapping samples</span></span>
<span id="cb19-274"><a href="#cb19-274" aria-hidden="true" tabindex="-1"></a>train_sampler <span class="op">=</span> DistributedSampler(train_dataset)</span>
<span id="cb19-275"><a href="#cb19-275" aria-hidden="true" tabindex="-1"></a>val_sampler <span class="op">=</span> DistributedSampler(</span>
<span id="cb19-276"><a href="#cb19-276" aria-hidden="true" tabindex="-1"></a>    val_dataset,</span>
<span id="cb19-277"><a href="#cb19-277" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle the validation dataset</span></span>
<span id="cb19-278"><a href="#cb19-278" aria-hidden="true" tabindex="-1"></a>    drop_last<span class="op">=</span><span class="va">True</span>, <span class="co"># DistributedSampler will append additional samples to fill an incomplete batch.  We don't want that for the validation dataset.</span></span>
<span id="cb19-279"><a href="#cb19-279" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-280"><a href="#cb19-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-281"><a href="#cb19-281" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb19-282"><a href="#cb19-282" aria-hidden="true" tabindex="-1"></a>    train_dataset,</span>
<span id="cb19-283"><a href="#cb19-283" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,  <span class="co"># don't shuffle if using DistributedSampler as that's done within the sampler</span></span>
<span id="cb19-284"><a href="#cb19-284" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>train_sampler,</span>
<span id="cb19-285"><a href="#cb19-285" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb19-286"><a href="#cb19-286" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb19-287"><a href="#cb19-287" aria-hidden="true" tabindex="-1"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-288"><a href="#cb19-288" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb19-289"><a href="#cb19-289" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-290"><a href="#cb19-290" aria-hidden="true" tabindex="-1"></a>val_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb19-291"><a href="#cb19-291" aria-hidden="true" tabindex="-1"></a>    val_dataset,</span>
<span id="cb19-292"><a href="#cb19-292" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb19-293"><a href="#cb19-293" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>val_sampler,</span>
<span id="cb19-294"><a href="#cb19-294" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span>num_workers_per_proc,</span>
<span id="cb19-295"><a href="#cb19-295" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size_per_proc,</span>
<span id="cb19-296"><a href="#cb19-296" aria-hidden="true" tabindex="-1"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-297"><a href="#cb19-297" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>collate_rowgroups,</span>
<span id="cb19-298"><a href="#cb19-298" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-299"><a href="#cb19-299" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-300"><a href="#cb19-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-301"><a href="#cb19-301" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 7. Shuffling across epochs</span></span>
<span id="cb19-302"><a href="#cb19-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-303"><a href="#cb19-303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calling the <span class="in">`set_epoch()`</span> method on the <span class="in">`DistributedSampler`</span> at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch.</span>
<span id="cb19-304"><a href="#cb19-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-307"><a href="#cb19-307" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-308"><a href="#cb19-308" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: shuffling across epochs</span></span>
<span id="cb19-309"><a href="#cb19-309" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-310"><a href="#cb19-310" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-311"><a href="#cb19-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-312"><a href="#cb19-312" aria-hidden="true" tabindex="-1"></a><span class="co"># ... Neural Network setup</span></span>
<span id="cb19-313"><a href="#cb19-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-314"><a href="#cb19-314" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb19-315"><a href="#cb19-315" aria-hidden="true" tabindex="-1"></a>    <span class="co"># need to call `set_epoch()` at the beginning of each epoch before creating the</span></span>
<span id="cb19-316"><a href="#cb19-316" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `DataLoader` iterator to make shuffling work properly across multiple epochs</span></span>
<span id="cb19-317"><a href="#cb19-317" aria-hidden="true" tabindex="-1"></a>    train_sampler.set_epoch(epch)</span>
<span id="cb19-318"><a href="#cb19-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-319"><a href="#cb19-319" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... training loop</span></span>
<span id="cb19-320"><a href="#cb19-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-321"><a href="#cb19-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-322"><a href="#cb19-322" aria-hidden="true" tabindex="-1"></a><span class="fu">### 6. Running the distributed training job</span></span>
<span id="cb19-323"><a href="#cb19-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-324"><a href="#cb19-324" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`rank`</span> is auto-allocated by DDP when calling <span class="co">[</span><span class="ot">`torch.multiprocessing.spawn`</span><span class="co">](https://pytorch.org/docs/stable/multiprocessing.html#spawning-subprocesses)</span>.</span>
<span id="cb19-325"><a href="#cb19-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`world_size`</span> is the number of processes across the training job. For GPU training, this corresponds to the number of GPUs in use, and each process works on a dedicated GPU.</span>
<span id="cb19-326"><a href="#cb19-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Both <span class="in">`rank`</span> and <span class="in">`world_size`</span> are new parameters to <span class="in">`main()`</span>.  Because of how spawning processes works, <span class="in">`rank`</span> _needs_ to be the first parameter to the calling function, <span class="in">`main(rank, ...)`</span>.</span>
<span id="cb19-327"><a href="#cb19-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-328"><a href="#cb19-328" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="PyTorch Multiprocessing"}</span>
<span id="cb19-329"><a href="#cb19-329" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">PyTorch's `torch.multiprocessing` package</span><span class="co">](https://pytorch.org/docs/stable/multiprocessing.html)</span> is a wrapper around the native <span class="in">`multiprocessing`</span> module and the API is 100% compatible.</span>
<span id="cb19-330"><a href="#cb19-330" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-331"><a href="#cb19-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-334"><a href="#cb19-334" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-335"><a href="#cb19-335" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: mean and variance</span></span>
<span id="cb19-336"><a href="#cb19-336" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-337"><a href="#cb19-337" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-338"><a href="#cb19-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-339"><a href="#cb19-339" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-340"><a href="#cb19-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-341"><a href="#cb19-341" aria-hidden="true" tabindex="-1"></a><span class="co"># ... CLI args parsing</span></span>
<span id="cb19-342"><a href="#cb19-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-343"><a href="#cb19-343" aria-hidden="true" tabindex="-1"></a>world_size <span class="op">=</span> torch.cuda.device_count()  <span class="co"># number of GPUs</span></span>
<span id="cb19-344"><a href="#cb19-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-345"><a href="#cb19-345" aria-hidden="true" tabindex="-1"></a>torch.multiprocessing.spawn(</span>
<span id="cb19-346"><a href="#cb19-346" aria-hidden="true" tabindex="-1"></a>    main,</span>
<span id="cb19-347"><a href="#cb19-347" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>(</span>
<span id="cb19-348"><a href="#cb19-348" aria-hidden="true" tabindex="-1"></a>        world_size,</span>
<span id="cb19-349"><a href="#cb19-349" aria-hidden="true" tabindex="-1"></a>        args.train_path,</span>
<span id="cb19-350"><a href="#cb19-350" aria-hidden="true" tabindex="-1"></a>        args.random_state,</span>
<span id="cb19-351"><a href="#cb19-351" aria-hidden="true" tabindex="-1"></a>        args.lr,</span>
<span id="cb19-352"><a href="#cb19-352" aria-hidden="true" tabindex="-1"></a>        args.epochs,</span>
<span id="cb19-353"><a href="#cb19-353" aria-hidden="true" tabindex="-1"></a>        args.num_workers,</span>
<span id="cb19-354"><a href="#cb19-354" aria-hidden="true" tabindex="-1"></a>        args.batch_size,</span>
<span id="cb19-355"><a href="#cb19-355" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb19-356"><a href="#cb19-356" aria-hidden="true" tabindex="-1"></a>    nprocs<span class="op">=</span>world_size,  <span class="co"># this is used to set the `rank` parameter.  It is passed as the first argument</span></span>
<span id="cb19-357"><a href="#cb19-357" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-358"><a href="#cb19-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-359"><a href="#cb19-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-360"><a href="#cb19-360" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLFlow logging</span></span>
<span id="cb19-361"><a href="#cb19-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-362"><a href="#cb19-362" aria-hidden="true" tabindex="-1"></a>Since there are now multiple processes runnning the same code, the same logging will happen on each process.  MLFlow doesn't know how to distinguish that there are different processes logging the same metric.  We can guard against this by only logging on the main process (GPU0):</span>
<span id="cb19-363"><a href="#cb19-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-366"><a href="#cb19-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-367"><a href="#cb19-367" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: MLFlow and torch.distributed</span></span>
<span id="cb19-368"><a href="#cb19-368" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-369"><a href="#cb19-369" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb19-370"><a href="#cb19-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-371"><a href="#cb19-371" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb19-372"><a href="#cb19-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-373"><a href="#cb19-373" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb19-374"><a href="#cb19-374" aria-hidden="true" tabindex="-1"></a>    mlflow.log_metrics(</span>
<span id="cb19-375"><a href="#cb19-375" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb19-376"><a href="#cb19-376" aria-hidden="true" tabindex="-1"></a>            <span class="st">"train loss"</span>: train_loss,</span>
<span id="cb19-377"><a href="#cb19-377" aria-hidden="true" tabindex="-1"></a>            <span class="st">"train accuracy"</span>: train_accuracy,</span>
<span id="cb19-378"><a href="#cb19-378" aria-hidden="true" tabindex="-1"></a>            <span class="st">"val loss"</span>: val_loss,</span>
<span id="cb19-379"><a href="#cb19-379" aria-hidden="true" tabindex="-1"></a>            <span class="st">"val accuracy"</span>: val_accuracy,</span>
<span id="cb19-380"><a href="#cb19-380" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb19-381"><a href="#cb19-381" aria-hidden="true" tabindex="-1"></a>        step<span class="op">=</span>epoch,</span>
<span id="cb19-382"><a href="#cb19-382" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-383"><a href="#cb19-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-384"><a href="#cb19-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-385"><a href="#cb19-385" aria-hidden="true" tabindex="-1"></a><span class="fu">## # Gradients, Losses, and Metrics</span></span>
<span id="cb19-386"><a href="#cb19-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-387"><a href="#cb19-387" aria-hidden="true" tabindex="-1"></a>Under the hood, DDP synchronizes and gathers the gradients across all processes.  However, any other ad-hoc value calculated in your code is not; e.g. losses and metrics.</span>
<span id="cb19-388"><a href="#cb19-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-389"><a href="#cb19-389" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Gradients are synchronized"}</span>
<span id="cb19-390"><a href="#cb19-390" aria-hidden="true" tabindex="-1"></a>Model gradients are synchronized across processes during the backward pass.  This means that the model in each process is the same!  <span class="co">[</span><span class="ot">See DDP: Internal Design</span><span class="co">](https://pytorch.org/docs/master/notes/ddp.html#internal-design)</span>.</span>
<span id="cb19-391"><a href="#cb19-391" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-392"><a href="#cb19-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-393"><a href="#cb19-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-394"><a href="#cb19-394" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution title="Losses are not synchronized"}</span>
<span id="cb19-395"><a href="#cb19-395" aria-hidden="true" tabindex="-1"></a>Even though the model is the same in each process, the loss is calculated on only the portion of the batch that each process sees.  The losses don't need to be synchronized for training but we may want to synchronize the losses for logging or definitelty when calculating metrics on the hold-out (validation) dataset.</span>
<span id="cb19-396"><a href="#cb19-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-397"><a href="#cb19-397" aria-hidden="true" tabindex="-1"></a>We could avoid this by _not_ using a `DistributedSampler` for the validation set, but then only 1 process would be used to calculate the loss for the whole validation set each epoch, which will be _slow_.</span>
<span id="cb19-398"><a href="#cb19-398" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-399"><a href="#cb19-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-400"><a href="#cb19-400" aria-hidden="true" tabindex="-1"></a>So if each process is calculating and accumulating losses and metrics separately, how do we log those and report as if there were a single process?  Well, those values will need to be gathered and then accumulated.  Say we have 4 processes, one for each GPU, and each is processing 1/4 of the training dataset.  We want to report the loss for each epoch.  If we log the loss in each process, we will have 4 different losses.  We can gather and combine them in a few ways.  Since the loss is just a number value we can use <span class="co">[</span><span class="ot">`torch.distributed.reduce`</span><span class="co">](https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce)</span> or <span class="co">[</span><span class="ot">`torch.distributed.all_reduce`</span><span class="co">](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce)</span>:</span>
<span id="cb19-401"><a href="#cb19-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-402"><a href="#cb19-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: `torch.distributed.reduce`</span></span>
<span id="cb19-403"><a href="#cb19-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-404"><a href="#cb19-404" aria-hidden="true" tabindex="-1"></a>In this example, we gather and combine using summation with the <span class="in">`dist.ReduceOp.SUM`</span>, all the <span class="in">`loss_tensor`</span>s into the process <span class="in">`0`</span> tensor (<span class="in">`dst=0`</span>).  Each tensor in each process must be the same shape.  Since we are assigning the values in each processes's <span class="in">`loss_tensor`</span> to it's rank, we expect the final gathered values to be <span class="in">`0 + 1 + 2 + 3 = 6`</span> in the main process <span class="in">`loss_tensor`</span>.</span>
<span id="cb19-405"><a href="#cb19-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-408"><a href="#cb19-408" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-409"><a href="#cb19-409" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: example `torch.distributed.reduce`</span></span>
<span id="cb19-410"><a href="#cb19-410" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-411"><a href="#cb19-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-412"><a href="#cb19-412" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-413"><a href="#cb19-413" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-414"><a href="#cb19-414" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb19-415"><a href="#cb19-415" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb19-416"><a href="#cb19-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-417"><a href="#cb19-417" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb19-418"><a href="#cb19-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-419"><a href="#cb19-419" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reduce_tensor(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-420"><a href="#cb19-420" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb19-421"><a href="#cb19-421" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb19-422"><a href="#cb19-422" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb19-423"><a href="#cb19-423" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb19-424"><a href="#cb19-424" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-425"><a href="#cb19-425" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb19-426"><a href="#cb19-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-427"><a href="#cb19-427" aria-hidden="true" tabindex="-1"></a>    loss_tensor <span class="op">=</span> torch.tensor([rank, rank]).cuda()</span>
<span id="cb19-428"><a href="#cb19-428" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb19-429"><a href="#cb19-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-430"><a href="#cb19-430" aria-hidden="true" tabindex="-1"></a>    dist.<span class="bu">reduce</span>(loss_tensor, op<span class="op">=</span>dist.ReduceOp.SUM, dst<span class="op">=</span><span class="dv">0</span>, async_op<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-431"><a href="#cb19-431" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb19-432"><a href="#cb19-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-433"><a href="#cb19-433" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb19-434"><a href="#cb19-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-435"><a href="#cb19-435" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb19-436"><a href="#cb19-436" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb19-437"><a href="#cb19-437" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb19-438"><a href="#cb19-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-439"><a href="#cb19-439" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb19-440"><a href="#cb19-440" aria-hidden="true" tabindex="-1"></a>    mp.spawn(reduce_tensor, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span>
<span id="cb19-441"><a href="#cb19-441" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-444"><a href="#cb19-444" aria-hidden="true" tabindex="-1"></a><span class="in">```{text}</span></span>
<span id="cb19-445"><a href="#cb19-445" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([1, 1], device='cuda:1')</span></span>
<span id="cb19-446"><a href="#cb19-446" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([3, 3], device='cuda:3')</span></span>
<span id="cb19-447"><a href="#cb19-447" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([2, 2], device='cuda:2')</span></span>
<span id="cb19-448"><a href="#cb19-448" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([0, 0], device='cuda:0')</span></span>
<span id="cb19-449"><a href="#cb19-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-450"><a href="#cb19-450" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([6, 6], device='cuda:0')</span></span>
<span id="cb19-451"><a href="#cb19-451" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([1, 1], device='cuda:1')</span></span>
<span id="cb19-452"><a href="#cb19-452" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([2, 2], device='cuda:2')</span></span>
<span id="cb19-453"><a href="#cb19-453" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([3, 3], device='cuda:3')</span></span>
<span id="cb19-454"><a href="#cb19-454" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-455"><a href="#cb19-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-456"><a href="#cb19-456" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: `torch.distributed.all_reduce`</span></span>
<span id="cb19-457"><a href="#cb19-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-458"><a href="#cb19-458" aria-hidden="true" tabindex="-1"></a>In this example, we gather and combine using summation with the <span class="in">`dist.ReduceOp.SUM`</span>, all the <span class="in">`loss_tensor`</span>s into all the processes.  Each tensor in each process must be the same shape.  Since we are assigning the values in each processes's <span class="in">`loss_tensor`</span> to it's rank, we expect the final gathered values to be <span class="in">`0 + 1 + 2 + 3 = 6`</span> in the all the processes's <span class="in">`loss_tensor`</span>.</span>
<span id="cb19-459"><a href="#cb19-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-462"><a href="#cb19-462" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-463"><a href="#cb19-463" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: example `torch.distributed.all_reduce`</span></span>
<span id="cb19-464"><a href="#cb19-464" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-465"><a href="#cb19-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-466"><a href="#cb19-466" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-467"><a href="#cb19-467" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-468"><a href="#cb19-468" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb19-469"><a href="#cb19-469" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb19-470"><a href="#cb19-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-471"><a href="#cb19-471" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb19-472"><a href="#cb19-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-473"><a href="#cb19-473" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_reduce_tensor(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-474"><a href="#cb19-474" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb19-475"><a href="#cb19-475" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb19-476"><a href="#cb19-476" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb19-477"><a href="#cb19-477" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb19-478"><a href="#cb19-478" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-479"><a href="#cb19-479" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb19-480"><a href="#cb19-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-481"><a href="#cb19-481" aria-hidden="true" tabindex="-1"></a>    loss_tensor <span class="op">=</span> torch.tensor([rank, rank]).cuda()</span>
<span id="cb19-482"><a href="#cb19-482" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb19-483"><a href="#cb19-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-484"><a href="#cb19-484" aria-hidden="true" tabindex="-1"></a>    dist.all_reduce(loss_tensor, op<span class="op">=</span>dist.ReduceOp.SUM, dst<span class="op">=</span><span class="dv">0</span>, async_op<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-485"><a href="#cb19-485" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss_tensor)</span>
<span id="cb19-486"><a href="#cb19-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-487"><a href="#cb19-487" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb19-488"><a href="#cb19-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-489"><a href="#cb19-489" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb19-490"><a href="#cb19-490" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb19-491"><a href="#cb19-491" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb19-492"><a href="#cb19-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-493"><a href="#cb19-493" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb19-494"><a href="#cb19-494" aria-hidden="true" tabindex="-1"></a>    mp.spawn(all_reduce_tensor, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span>
<span id="cb19-495"><a href="#cb19-495" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-498"><a href="#cb19-498" aria-hidden="true" tabindex="-1"></a><span class="in">```{text}</span></span>
<span id="cb19-499"><a href="#cb19-499" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([2, 2], device='cuda:2')</span></span>
<span id="cb19-500"><a href="#cb19-500" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([3, 3], device='cuda:3')</span></span>
<span id="cb19-501"><a href="#cb19-501" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([1, 1], device='cuda:1')</span></span>
<span id="cb19-502"><a href="#cb19-502" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([0, 0], device='cuda:0')</span></span>
<span id="cb19-503"><a href="#cb19-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-504"><a href="#cb19-504" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([6, 6], device='cuda:2')</span></span>
<span id="cb19-505"><a href="#cb19-505" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([6, 6], device='cuda:3')</span></span>
<span id="cb19-506"><a href="#cb19-506" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([6, 6], device='cuda:1')</span></span>
<span id="cb19-507"><a href="#cb19-507" aria-hidden="true" tabindex="-1"></a><span class="in">tensor([6, 6], device='cuda:0')</span></span>
<span id="cb19-508"><a href="#cb19-508" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-509"><a href="#cb19-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-510"><a href="#cb19-510" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: `torch.distributed.gather_object`</span></span>
<span id="cb19-511"><a href="#cb19-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-512"><a href="#cb19-512" aria-hidden="true" tabindex="-1"></a>Syncing across processing is simple enough for tensors, but if we have a number of values to gather (say a bunch of metrics for example) it would be easier to only need to gather once and store those values in an appropriate data structure.  Most of the distributed gathering function only work on tensors, but we can use <span class="co">[</span><span class="ot">`torch.distributed.gather_object`</span><span class="co">](https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather_object)</span> and / or <span class="co">[</span><span class="ot">`torch.distributed.all_gather_object`</span><span class="co">](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object)</span> to pass pickleable Python objects between ranks.</span>
<span id="cb19-513"><a href="#cb19-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-514"><a href="#cb19-514" aria-hidden="true" tabindex="-1"></a>In this example, we want to track the losses and number of samples in a <span class="in">`Counter`</span> so that we can combine and calculate the mean loss after gathering.  We gather each loss counter to rank 0.  Each counter is placed into the <span class="in">`gather_list`</span>, which must have all elements set to <span class="in">`None`</span> initially.  When calling, <span class="in">`dist.gather_object`</span>, the <span class="in">`gather_list`</span> must only exist in the rank being gathered to (<span class="in">`dst=0`</span> or rank 0 in this case).  Then we use <span class="in">`functools.reduce`</span> to sum all the Counters gathered in the <span class="in">`gather_list`</span>.</span>
<span id="cb19-515"><a href="#cb19-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-518"><a href="#cb19-518" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-519"><a href="#cb19-519" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: example `torch.distributed.gather_object`</span></span>
<span id="cb19-520"><a href="#cb19-520" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-521"><a href="#cb19-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-522"><a href="#cb19-522" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-523"><a href="#cb19-523" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> operator</span>
<span id="cb19-524"><a href="#cb19-524" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb19-525"><a href="#cb19-525" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-526"><a href="#cb19-526" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb19-527"><a href="#cb19-527" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb19-528"><a href="#cb19-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-529"><a href="#cb19-529" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb19-530"><a href="#cb19-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-531"><a href="#cb19-531" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gather_object(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-532"><a href="#cb19-532" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb19-533"><a href="#cb19-533" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb19-534"><a href="#cb19-534" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb19-535"><a href="#cb19-535" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb19-536"><a href="#cb19-536" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-537"><a href="#cb19-537" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb19-538"><a href="#cb19-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-539"><a href="#cb19-539" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> Counter(</span>
<span id="cb19-540"><a href="#cb19-540" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"loss"</span>: <span class="fl">0.01</span>, <span class="st">"num_samples"</span>: rank}</span>
<span id="cb19-541"><a href="#cb19-541" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-542"><a href="#cb19-542" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb19-543"><a href="#cb19-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-544"><a href="#cb19-544" aria-hidden="true" tabindex="-1"></a>    gather_list <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb19-545"><a href="#cb19-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-546"><a href="#cb19-546" aria-hidden="true" tabindex="-1"></a>    dist.gather_object(losses, gather_list <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">None</span>, dst<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-547"><a href="#cb19-547" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> functools.<span class="bu">reduce</span>(operator.add, gather_list)</span>
<span id="cb19-548"><a href="#cb19-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-549"><a href="#cb19-549" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb19-550"><a href="#cb19-550" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb19-551"><a href="#cb19-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-552"><a href="#cb19-552" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb19-553"><a href="#cb19-553" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb19-554"><a href="#cb19-554" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb19-555"><a href="#cb19-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-556"><a href="#cb19-556" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb19-557"><a href="#cb19-557" aria-hidden="true" tabindex="-1"></a>    mp.spawn(gather_object, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span>
<span id="cb19-558"><a href="#cb19-558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-561"><a href="#cb19-561" aria-hidden="true" tabindex="-1"></a><span class="in">```{text}</span></span>
<span id="cb19-562"><a href="#cb19-562" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 1, 'loss': 0.01})</span></span>
<span id="cb19-563"><a href="#cb19-563" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 2, 'loss': 0.01})</span></span>
<span id="cb19-564"><a href="#cb19-564" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 4, 'loss': 0.01})</span></span>
<span id="cb19-565"><a href="#cb19-565" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 3, 'loss': 0.01})</span></span>
<span id="cb19-566"><a href="#cb19-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-567"><a href="#cb19-567" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 10, 'loss': 0.04})</span></span>
<span id="cb19-568"><a href="#cb19-568" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 4, 'loss': 0.01})</span></span>
<span id="cb19-569"><a href="#cb19-569" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 3, 'loss': 0.01})</span></span>
<span id="cb19-570"><a href="#cb19-570" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 2, 'loss': 0.01})</span></span>
<span id="cb19-571"><a href="#cb19-571" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-572"><a href="#cb19-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-573"><a href="#cb19-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-574"><a href="#cb19-574" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: `torch.distributed.all_gather_object`</span></span>
<span id="cb19-575"><a href="#cb19-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-576"><a href="#cb19-576" aria-hidden="true" tabindex="-1"></a>In this example, we want to track the losses and number of samples in a <span class="in">`Counter`</span> so that we can combine and calculate the mean loss after gathering.  We gather each loss counter to each rank.  Each counter is placed into the <span class="in">`gather_list`</span>, which must have all elements set to <span class="in">`None`</span> initially.  Then we use <span class="in">`functools.reduce`</span> to sum all the Counters gathered in the <span class="in">`gather_list`</span>.</span>
<span id="cb19-577"><a href="#cb19-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-580"><a href="#cb19-580" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-581"><a href="#cb19-581" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: example `torch.distributed.all_gather_object`</span></span>
<span id="cb19-582"><a href="#cb19-582" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb19-583"><a href="#cb19-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-584"><a href="#cb19-584" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-585"><a href="#cb19-585" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> operator</span>
<span id="cb19-586"><a href="#cb19-586" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb19-587"><a href="#cb19-587" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-588"><a href="#cb19-588" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb19-589"><a href="#cb19-589" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb19-590"><a href="#cb19-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-591"><a href="#cb19-591" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group</span>
<span id="cb19-592"><a href="#cb19-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-593"><a href="#cb19-593" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_gather_object(rank: <span class="bu">int</span>, world_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-594"><a href="#cb19-594" aria-hidden="true" tabindex="-1"></a>    init_process_group(</span>
<span id="cb19-595"><a href="#cb19-595" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"gloo"</span>,  <span class="co"># CPU only works on gloo backend</span></span>
<span id="cb19-596"><a href="#cb19-596" aria-hidden="true" tabindex="-1"></a>        rank<span class="op">=</span>rank,</span>
<span id="cb19-597"><a href="#cb19-597" aria-hidden="true" tabindex="-1"></a>        world_size<span class="op">=</span>world_size,</span>
<span id="cb19-598"><a href="#cb19-598" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-599"><a href="#cb19-599" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(rank) <span class="co"># tell each device (GPU) which one it is.</span></span>
<span id="cb19-600"><a href="#cb19-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-601"><a href="#cb19-601" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> Counter(</span>
<span id="cb19-602"><a href="#cb19-602" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"loss"</span>: <span class="fl">0.01</span>, <span class="st">"num_samples"</span>: rank}</span>
<span id="cb19-603"><a href="#cb19-603" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-604"><a href="#cb19-604" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb19-605"><a href="#cb19-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-606"><a href="#cb19-606" aria-hidden="true" tabindex="-1"></a>    gather_list <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb19-607"><a href="#cb19-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-608"><a href="#cb19-608" aria-hidden="true" tabindex="-1"></a>    dist.all_gather_object(gather_list, losses)</span>
<span id="cb19-609"><a href="#cb19-609" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> functools.<span class="bu">reduce</span>(operator.add, gather_list)</span>
<span id="cb19-610"><a href="#cb19-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-611"><a href="#cb19-611" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(losses)</span>
<span id="cb19-612"><a href="#cb19-612" aria-hidden="true" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb19-613"><a href="#cb19-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-614"><a href="#cb19-614" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb19-615"><a href="#cb19-615" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb19-616"><a href="#cb19-616" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"12345"</span></span>
<span id="cb19-617"><a href="#cb19-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-618"><a href="#cb19-618" aria-hidden="true" tabindex="-1"></a>    num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb19-619"><a href="#cb19-619" aria-hidden="true" tabindex="-1"></a>    mp.spawn(all_gather_object, nprocs<span class="op">=</span>num_gpu, args<span class="op">=</span>(num_gpu,))</span>
<span id="cb19-620"><a href="#cb19-620" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-623"><a href="#cb19-623" aria-hidden="true" tabindex="-1"></a><span class="in">```{text}</span></span>
<span id="cb19-624"><a href="#cb19-624" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 1, 'loss': 0.01})</span></span>
<span id="cb19-625"><a href="#cb19-625" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 4, 'loss': 0.01})</span></span>
<span id="cb19-626"><a href="#cb19-626" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 2, 'loss': 0.01})</span></span>
<span id="cb19-627"><a href="#cb19-627" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 3, 'loss': 0.01})</span></span>
<span id="cb19-628"><a href="#cb19-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-629"><a href="#cb19-629" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 10, 'loss': 0.04})</span></span>
<span id="cb19-630"><a href="#cb19-630" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 10, 'loss': 0.04})</span></span>
<span id="cb19-631"><a href="#cb19-631" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 10, 'loss': 0.04})</span></span>
<span id="cb19-632"><a href="#cb19-632" aria-hidden="true" tabindex="-1"></a><span class="in">Counter({'num_samples': 10, 'loss': 0.04})</span></span>
<span id="cb19-633"><a href="#cb19-633" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-634"><a href="#cb19-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-635"><a href="#cb19-635" aria-hidden="true" tabindex="-1"></a><span class="fu">## # Multi-Node Training</span></span>
<span id="cb19-636"><a href="#cb19-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-637"><a href="#cb19-637" aria-hidden="true" tabindex="-1"></a>Now that we've setup DDP, we have the option of running on multiple nodes.  There are a few extra bits that need to be changed before we run <span class="co">[</span><span class="ot">multinode training</span><span class="co">](https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html)</span>:</span>
<span id="cb19-638"><a href="#cb19-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-639"><a href="#cb19-639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>change a few lines to enable <span class="co">[</span><span class="ot">`torchrun`</span><span class="co">](https://pytorch.org/docs/stable/elastic/run.html)</span></span>
<span id="cb19-640"><a href="#cb19-640" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>install the project as a python package for <span class="in">`torchrun`</span> to work properly</span>
<span id="cb19-641"><a href="#cb19-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>add <span class="co">[</span><span class="ot">fault tolerance</span><span class="co">](https://pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html)</span></span>
<span id="cb19-642"><a href="#cb19-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-643"><a href="#cb19-643" aria-hidden="true" tabindex="-1"></a><span class="fu">## # Resources</span></span>
<span id="cb19-644"><a href="#cb19-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-645"><a href="#cb19-645" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">`DistributedDataParallel`</span><span class="co">](https://pytorch.org/docs/stable/notes/ddp.html)</span> - Documentation for DDP.</span>
<span id="cb19-646"><a href="#cb19-646" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="co">[</span><span class="ot">Getting Started with Distributed Data Parallel</span><span class="co">](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)</span> - Good starting point to understand DDP and writing a training script using DDP for single-node and multi-node.</span>
<span id="cb19-647"><a href="#cb19-647" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="co">[</span><span class="ot">Writing Distributed Applications with Pytorch</span><span class="co">](https://pytorch.org/tutorials/intermediate/dist_tuto.html)</span> - In-depth article about writing distributed applications in PyTorch and how communication works under the hood.</span>
<span id="cb19-648"><a href="#cb19-648" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span><span class="co">[</span><span class="ot">`DistributedSampler`</span><span class="co">](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler)</span> - Used in conjunction with a DataLoader, the DistributedSampler enables each process to only load the data it processes, rather than all the data to be processed.</span>
<span id="cb19-649"><a href="#cb19-649" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span><span class="co">[</span><span class="ot">DistributedDataParallel training in PyTorch</span><span class="co">](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)</span> - Explaination of how DDP works and how to use it.</span>
<span id="cb19-650"><a href="#cb19-650" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span><span class="co">[</span><span class="ot">(AML) Distributed GPU training guide (SDK v2)</span><span class="co">](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-distributed-gpu?view=azureml-api-2)</span></span>
<span id="cb19-651"><a href="#cb19-651" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span><span class="co">[</span><span class="ot">PyTorch DistributedDataParallel Example In Azure ML - Multi-Node Multi-GPU Distributed Training</span><span class="co">](https://ochzhen.com/blog/pytorch-distributed-data-parallel-azure-ml)</span> - Example of DDP for AML.</span>
<span id="cb19-652"><a href="#cb19-652" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span><span class="co">[</span><span class="ot">PyTorch Distributed: Experiences on Accelerating Data Parallel Training</span><span class="co">](https://arxiv.org/pdf/2006.15704)</span> - Paper on how Facebook designed DDP to be faster for distributed training.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../LICENSE.html">
<p>License</p>
</a>
  </li>  
</ul>
    <div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>