[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Robert",
    "section": "",
    "text": "I was once nicknamed Beartoes through a chain of thought multiple links long. It’s grown on me.\nRobert |&gt; Bert |&gt; Berto |&gt; Beartoes"
  },
  {
    "objectID": "posts/waltzing_with_the_walrus/index.html",
    "href": "posts/waltzing_with_the_walrus/index.html",
    "title": "Waltzing with Python’s Walrus Operator",
    "section": "",
    "text": "Python 3.8 introduced a new assignment operator with PEP 572 called assignment expressions, a.k.a the walrus operator. The walrus operator uses the new walrus-like syntax :=, to assign variables within an expression.\nIt’s been out for a few years at this point (at the time of writing Python 3.12 is around the corner) and I’ve found some joy in how it’s helped elegantly shorten some parts of my code. Here are the ways I’ve made use of the walrus operator."
  },
  {
    "objectID": "posts/waltzing_with_the_walrus/index.html#error-handling",
    "href": "posts/waltzing_with_the_walrus/index.html#error-handling",
    "title": "Waltzing with Python’s Walrus Operator",
    "section": "Error Handling",
    "text": "Error Handling\nThe walrus operator can help reduce repetition and make error handling a bit more streamlined. In the following snippet, func() will return None to represent an error occurred.\nx = func()\nif not x:\n    print(\"Error message\")\n    return\nUsing the walrus operator, the call to func() can be inlined.\nif not x := func():\n    print(\"Error message\")\n    return\nShaving off a single line may seem trivial but those saved lines can add up. For example, when parsing user inputs and performing validation. In the following snippet, we want to validate user inputs x, y, and z. If there’s a validation problem, validate will return a string with a message explaining what is wrong with the input and a message of how to fix. These messages get appended to a list so all validation messages can be printed out together.\nvalidation_errors = []\nif msg := validate(x):\n    validation_errors += msg\n\nif msg := validate(y):\n    validation_errors += msg\n\nif msg := validate(z):\n    validation_errors += msg\n\nif validation_errors: # a non-empty list resolves to True\n    for error in validation_errors:\n        print(error)\n    return"
  },
  {
    "objectID": "posts/waltzing_with_the_walrus/index.html#comprehensions",
    "href": "posts/waltzing_with_the_walrus/index.html#comprehensions",
    "title": "Waltzing with Python’s Walrus Operator",
    "section": "Comprehensions",
    "text": "Comprehensions\nLet’s say we wanted to create a list of results from expensive function call but only results that aren’t None. With a list comprehension, the expensive function would need to be called twice. Not ideal.\ny = [\n    expensive_function(i) if expensive_function(i) for i in range(0, 10)\n]\nOf course, you could use normal for loop syntax but it’s a fair bit more verbose, and for illustrative purposes, we’re allergic to verbose.\ny = []\nfor a in range(0, 10):\n    x = expensive_function(i)\n    if x == 0:\n        y.append(x)\nThe walrus operator plops to the rescue here and allows us to use a list comprehension.\ny = [\n    x if (x := expensive_function(i)) for i in range(0, 10)\n]\nThis also applies to dictionary comprehensions.\ny = {\n    i: x if (x := expensive_function(i)) for i in range(0, 10)\n}"
  },
  {
    "objectID": "posts/waltzing_with_the_walrus/index.html#do-while-loops",
    "href": "posts/waltzing_with_the_walrus/index.html#do-while-loops",
    "title": "Waltzing with Python’s Walrus Operator",
    "section": "Do While Loops",
    "text": "Do While Loops\nA do-while loop was proposed for Python in PEP 315 but was rejected for not providing a material improvement over the following:\nwhile True:\n    x = f(a, b) # setup code\n    if not x:   # condition\n        break\n    # loop body using x\nA shortened version of do-while loop can be accomplished by having setup code execute once before the loop and moving the condition into a while loop. However, this is error-prone; x = f(a, b) is duplicated for both the setup code and the loop body, and if it needs changing there are now multiple places that must be updated.\nx = f(a, b) # setup code\nwhile x:    # condition\n    x = f(a, b)\n    # loop body using x\nWith the walrus operator, it can all be inlined to the while condition.\nwhile x := f(a, b): # setup code and condition\n    # loop body using x"
  },
  {
    "objectID": "posts/waltzing_with_the_walrus/index.html#pattern-matching",
    "href": "posts/waltzing_with_the_walrus/index.html#pattern-matching",
    "title": "Waltzing with Python’s Walrus Operator",
    "section": "Pattern Matching",
    "text": "Pattern Matching\nThe walrus operator can also be useful in Pattern Matching. Structural Pattern Matching was introduced in Python 3.10 with PEP 622. If you’re not yet familiar see PEP 363 for a tutorial. The walrus can be useful to inline a function call and store the return value in a variable for use in the cases.\nmatch x := f(a, b):\n    case 0:\n        # do stuff with x\n    case 1:\n        # do more stuff with x\n    case 2:\n        # even more doing with x"
  },
  {
    "objectID": "posts/waltzing_with_the_walrus/index.html#an-over-the-top-overuse-example",
    "href": "posts/waltzing_with_the_walrus/index.html#an-over-the-top-overuse-example",
    "title": "Waltzing with Python’s Walrus Operator",
    "section": "An Over-the-Top Overuse Example",
    "text": "An Over-the-Top Overuse Example\nWhile the walrus operator is handy for shaving off a few lines of code, inlining too much can make code difficult to reason about. Use it sparingly, especially with other code-golfing operators. For example, with the ternary operator.\nheight = get_height(name) if (name := get_name(user_id)) else None\nI think this can be okay but I also think it’s clearer written long-form,\nif name := get_name(user_id):\n    height = get_height(name)\nelse:\n    height = None\nIt could be formatted over multiple lines so it’s just as readible as a normal if / else and to keep the benefits of the ternary usage by only assigning height once, but it’s now a whopping 5 lines.\nheight = (\n    get_height(name)\n    if (name := get_name(user_id))\n    else None\n)\nAnd remember, just because you can doesn’t mean you should write code like below, if you can avoid it. This example is modified from my own code.\nparams: Dict[str, Dict[str, float]]\n\nsampler_weights = (\n    {\n        ModeEnum(mode): weight\n        for mode, weight in normalize_weights(weights).items()\n    }\n    if (weights := params.get(\"sampler\", {}).get(\"weights\"))\n    else {\"x\": 0.5, \"y\": 0.5}\n)\nHere, I’ve slapped a dictionary comprehension, a ternary operator, and a walrus operator into the same expression. There’s a lot going on, but it’s formatted over multiple lines to help delineate what’s happening. The variable, params, holds the contents of a configuration .toml that I needed to parse some weights from and convert into an dictionary of {enum: weight}."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Bear’s Toes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nCategories\n\n\n\n\n\n\n2024-01-14\n\n\nPub Quiz: Summing Two Random Variables\n\n\nMy company’s holiday pub quiz almost had a question about summing two random variables.\n\n\nprobability, pub quiz\n\n\n\n\n2023-08-13\n\n\nWaltzing with Python’s Walrus Operator\n\n\nUsing the walrus operator := to make life simpler.\n\n\npython\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/summing_random_variables/index.html",
    "href": "posts/summing_random_variables/index.html",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "",
    "text": "Last week, my company held a pub quiz. This was no ordinary pub quiz, it was designed in a way for teams to learn about other teams. One person from each team submitted four questions about their team. We ended up wtih about 60 questions from all across the company. And when I say “all” I mean all. We had questions about Data Science, Commercial, Operations, Engineering, Product, Marketing, Leadership, Human Resources, you name it. As someone who is fairly familiar with all aspects of our products and engineering teams (but by no means an expert in all these areas) by having been here over four years and written my share of documentation, I came into the pub quiz with confidence. Much of that confidence due to expecting to not need to know British pop culture back through the 70s as is standard in an actual pub quiz.\nThat confidence was quickly smashed with questions like:\n\nWhat was the 2023 Coca Cola Christmas commercial slogan?\nWhat is the collective age of the Customer Support team?\nWhat month was repo XXX created?\n\nThe Data Science round was the trickiest of them all, in no small part because reading the question took up almost all of the response time! After the quiz, my teammate who wrote the Data Science questions shared one that didn’t quite make the cut.\nI share the question with the reader here, with only some minor rewording. My answer follows."
  },
  {
    "objectID": "posts/summing_random_variables/index.html#question",
    "href": "posts/summing_random_variables/index.html#question",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# Question",
    "text": "# Question\n\n## The preamble\nYou fit the following univariate regression model using Ordinary Least Squares (OLS)\n\n\nIn the original question, the OLS abbreviation was given without the unabbreviated form. Not sure it would have made the question any easier.\n\\[\ny = \\alpha + \\beta x + \\epsilon\n\\]\nWhere the residual, \\(\\epsilon\\), is Normally distributed with mean 0 and standard deviation 1, that is:\n\\[\n\\epsilon \\sim {N}(0, 1)\n\\]\nFrom OLS, you determine:\n\n\\(\\alpha=3\\)\n\\(\\beta=2\\)\n\n\nThe next month, your colleague reruns the experiment and collects the same size dataset. They forgot to check the callibration of the machine used to collect the data and as a result the dataset now has a measurement error, \\(u\\). That is,\n\\[\n\\displaylines{\n    \\begin{align}\n    y^* &\\equiv y + u \\\\\n    y^* &= \\alpha^* + \\beta^*x + \\epsilon^*\n    \\end{align}\n}\n\\]\nThe measurement error has mean 2 and standard deviation 2.\n\\[\nu \\sim N(2,2)\n\\]\nThe residuals in the new sample dataset are also normally distributed.\n\\[\n\\epsilon^* \\sim {N}(\\mu^*, \\sigma^*)\n\\]\n\n\n\n## And finally the question\nWhat values does your colleague find when running a regression with OLS? (Try solving with a pen and paper):\n\n\\(\\alpha^* = \\text{ ??}\\)\n\\(\\beta^* = \\text{ ??}\\)\n\\(\\mu^* = \\text{ ??}\\)\n\\(\\sigma^* = \\text{ ??}\\)"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#answer",
    "href": "posts/summing_random_variables/index.html#answer",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# Answer",
    "text": "# Answer\nThere’s a few things to immediately note:\n\nThe hell this is a pub quiz question!\n\nWe assume \\(y^*\\), \\(\\epsilon\\), \\(u\\), and \\(\\epsilon^*\\) are independent random variables. If they weren’t, this would be a wee bit trickier (we would need to have information about the joint distributions, i.e. covariances).\nWe recognize that the new residuals are going to include the previous residuals as well as the new measurement error, \\(\\epsilon^* = \\epsilon + u\\).\nWhether it was a typo or meant to be tricky, the question states the standard deviation rather than the variance for the Normal distribution. Typically, the notation is \\(N(\\mu, \\sigma^2)\\). Rewriting into the standard notation:\n\n\n\nI’ve highly probably in all likelihood had the same question on a Probability exam during my Statistics postgrad.\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\epsilon &\\sim N(0, 1) && \\rightarrow N(0, 1) \\\\\n    u &\\sim N(2, 2) && \\rightarrow N(2, 4) \\\\\n    \\epsilon^* &\\sim N(\\mu^*, \\sigma^{*}) && \\rightarrow N(\\mu^*, \\sigma^{*2})\n    \\end{alignat*}\n}\n\\]\n\nThe problem boils down to realizing that we have a summation of two independent Normally distributed random variables, \\(y\\) and \\(u\\). How do independent Normally distributed random variables sum? If you don’t remember, don’t worry, neither did I. After some revision we know that (Ross 2010):\n\\[\n\\displaylines{\n    \\begin{align}\n    \\text{E}[X + Y]   &= \\text{E}[X] + \\text{E}[Y] \\\\\n    \\text{Var}[X + Y] &= \\text{Var}[X] + \\text{Var}[Y] + 2\\text{Cov}[X,Y]\n    \\end{align}\n}\n\\]\nThe assumption that the random variables are independent means the covariance is zero, \\(2\\text{Cov}[X,Y] = 0\\). We can simply add the means and variances (Ross 2010, 256–57) giving:\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    y^* &= y &&+ u \\\\\n        &= \\alpha + \\beta x &&+ \\epsilon + u \\\\\n        &= \\alpha + \\beta x &&+ \\epsilon^* \\\\\n        &= \\alpha + \\beta x &&+ \\Big( N(\\mu_{\\epsilon}, \\sigma_{\\epsilon}^2) + N(\\mu_u, \\sigma_u^2) \\Big) \\\\\n        &= \\alpha + \\beta x &&+ \\Big( N(\\mu_{\\epsilon} + \\mu_u, \\sigma_{\\epsilon}^2 + \\sigma_u^2) \\Big) \\\\\n        &= \\alpha + \\beta x &&+ N(0 + 2, 1 + 4) \\\\\n        &= \\alpha + \\beta x &&+ N(2, 5) \\\\\n    \\end{alignat*}\n}\n\\]\nwhere,\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\mu_{\\epsilon^*} &= \\mu_{\\epsilon} &&+ \\mu_u \\\\\n        &= 0 &&+ 2 \\\\\n        &= 2\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\sigma_{\\epsilon^*}^2 &= \\sigma_{\\epsilon}^2 &&+ \\sigma_u^2 \\\\\n        &= 1 &&+ 4 \\\\\n        &= 5\n    \\end{alignat*}\n}\n\\]\n\n\n\n\nPlugging in the values for \\(\\alpha\\) and \\(\\beta\\), we find the following solution: \\[\ny^* = 3 + 2x + N(2, 5)\n\\]\n\n\\(\\alpha^* = 3\\)\n\\(\\beta^* = 2\\)\n\\(u^* = 2\\)\n\\(\\sigma^* = \\sqrt{5}\\)\n\n\n\n\nRemember, we were asked for \\(\\sigma^*\\) not \\(\\sigma^{*2}\\).\nWe can arguably simplify this a bit further by noting that the bias, \\(\\alpha\\), and the mean of the residuals, \\(\\mu_{\\epsilon^*}\\), are both constants that shift the intercept and can be grouped. Subtracting the mean of the error from the bias…\n\\[\n\\displaylines{\n    \\begin{align}\n    y^* &= \\alpha + \\beta x + N(2, 5) \\\\\n        &= 3 + 2x + N(2, 5) \\\\\n        &= (3-2) + 2x + N(0, 5) \\\\\n        &= 1 + 2x + N(0,5)\n    \\end{align}\n}\n\\]\nAnd rewritting using the standard deviation rather than the variance (like in the original question) we arrive at:\n\\[\ny = 1 + 2x + N(0,\\sqrt{5})\n\\]\n\n\\(\\alpha^* = 1\\)\n\\(\\beta^* = 2\\)\n\\(u^* = 2\\)\n\\(\\sigma^* = \\sqrt{5}\\)"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#section",
    "href": "posts/summing_random_variables/index.html#section",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "—",
    "text": "—\nThe company pub quiz was neck and neck between 3 teams for most of the game. My team bouncing between \\(\\text{1}^{st}\\) and \\(\\text{3}^{rd}\\) place. That is until the final round of questions from the Leadership team.\nGood thing I had the CEO on my team 🥇"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#my-solution",
    "href": "posts/summing_random_variables/index.html#my-solution",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# My solution",
    "text": "# My solution\nThere’s a few things to immediately note:\n\nThe hell this is a pub quiz question!\n\nWe assume \\(y^*\\), \\(\\epsilon\\), \\(u\\), and \\(\\epsilon^*\\) are independent random variables. If they weren’t, this would be a wee bit trickier (we would need to have information about the joint distributions, i.e. covariances).\nWe recognize that the new residuals are going to include the previous residuals as well as the new measurement error, \\(\\epsilon^* = \\epsilon + u\\).\nWhether it was a typo or meant to be tricky, the question states the standard deviation rather than the variance for the Normal distribution. Typically, the notation is \\(N(\\mu, \\sigma^2)\\). Rewriting into the standard notation:\n\n\n\nI’ve highly probably in all likelihood had the same question on a Probability exam during my Statistics postgrad.\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\epsilon &\\sim N(0, 1) && \\rightarrow N(0, 1) \\\\\n    u &\\sim N(2, 2) && \\rightarrow N(2, 4) \\\\\n    \\epsilon^* &\\sim N(\\mu^*, \\sigma^{*}) && \\rightarrow N(\\mu^*, \\sigma^{*2})\n    \\end{alignat*}\n}\n\\]\n\nThe problem boils down to realizing that we have a summation of two independent Normally distributed random variables, \\(y\\) and \\(u\\). How do independent Normally distributed random variables sum? If you don’t remember, don’t worry, neither did I. After some revision we know that (Ross 2010):\n\\[\n\\displaylines{\n    \\begin{align}\n    \\text{E}[X + Y]   &= \\text{E}[X] + \\text{E}[Y] \\\\\n    \\text{Var}[X + Y] &= \\text{Var}[X] + \\text{Var}[Y] + 2\\text{Cov}[X,Y]\n    \\end{align}\n}\n\\]\nThe assumption that the random variables are independent means the covariance is zero, \\(2\\text{Cov}[X,Y] = 0\\). We can simply add the means and variances (Ross 2010, 256–57) giving:\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    y^* &= y &&+ u \\\\\n        &= \\alpha + \\beta x &&+ \\epsilon + u \\\\\n        &= \\alpha + \\beta x &&+ \\epsilon^* \\\\\n        &= \\alpha + \\beta x &&+ \\Big( N(\\mu_{\\epsilon}, \\sigma_{\\epsilon}^2) + N(\\mu_u, \\sigma_u^2) \\Big) \\\\\n        &= \\alpha + \\beta x &&+ \\Big( N(\\mu_{\\epsilon} + \\mu_u, \\sigma_{\\epsilon}^2 + \\sigma_u^2) \\Big) \\\\\n        &= \\alpha + \\beta x &&+ N(0 + 2, 1 + 4) \\\\\n        &= \\alpha + \\beta x &&+ N(2, 5) \\\\\n    \\end{alignat*}\n}\n\\]\nwhere,\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\mu_{\\epsilon^*} &= \\mu_{\\epsilon} &&+ \\mu_u \\\\\n        &= 0 &&+ 2 \\\\\n        &= 2\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\sigma_{\\epsilon^*}^2 &= \\sigma_{\\epsilon}^2 &&+ \\sigma_u^2 \\\\\n        &= 1 &&+ 4 \\\\\n        &= 5\n    \\end{alignat*}\n}\n\\]\n\n\n\n\nPlugging in the values for \\(\\alpha\\) and \\(\\beta\\), we find the following solution: \\[\ny^* = 3 + 2x + N(2, 5)\n\\]\n\n\\(\\alpha^* = 3\\)\n\\(\\beta^* = 2\\)\n\\(u^* = 2\\)\n\\(\\sigma^* = \\sqrt{5}\\)\n\n\n\n\nRemember, we were asked for \\(\\sigma^*\\) not \\(\\sigma^{*2}\\).\nWe can arguably simplify this a bit further by noting that the bias, \\(\\alpha\\), and the mean of the residuals, \\(\\mu_{\\epsilon^*}\\), are both constants that shift the intercept and can be grouped. Subtracting the mean of the error from the bias…\n\\[\n\\displaylines{\n    \\begin{alignat*}{3}\n    y^* &= \\alpha &&+ \\beta x &&&+ N(2, 5) \\\\\n        &= 3      &&+ 2x      &&&+ N(2, 5) \\\\\n        &= (3-2)  &&+ 2x      &&&+ N(0, 5) \\\\\n        &= 1      &&+ 2x      &&&+ N(0,5)\n    \\end{alignat*}\n}\n\\]\nAnd rewritting using the standard deviation rather than the variance (like in the original question) we arrive at:\n\\[\ny = 1 + 2x + N(0,\\sqrt{5})\n\\]\n\n\\(\\alpha^* = 1\\)\n\\(\\beta^* = 2\\)\n\\(u^* = 2\\)\n\\(\\sigma^* = \\sqrt{5}\\)"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#analytical-solution",
    "href": "posts/summing_random_variables/index.html#analytical-solution",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# Analytical solution",
    "text": "# Analytical solution\nThere’s a few things to immediately note:\n\nThe hell this is a pub quiz question!\n\nWe assume \\(y^*\\), \\(\\epsilon\\), \\(u\\), and \\(\\epsilon^*\\) are independent random variables. If they weren’t, this would be a wee bit trickier (we would need to have information about the joint distributions, i.e. covariances).\nWe recognize that the new residuals are going to include the previous residuals as well as the new measurement error, \\(\\epsilon^* = \\epsilon + u\\).\nWhether it was a typo or meant to be tricky, the question states the standard deviation rather than the variance for the Normal distribution. Typically, the notation is uses the variance, \\(N(\\mu, \\sigma^2)\\). Rewriting into the standard notation:\n\n\n\nI’ve highly probably in all likelihood had the same question on a Probability exam during my Statistics postgrad.\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\epsilon &\\sim N(0, 1) && \\rightarrow N(0, 1) \\\\\n    u &\\sim N(2, 2) && \\rightarrow N(2, \\sqrt{2}) \\\\\n    \\epsilon^* &\\sim N(\\mu^*, \\sigma^{*}) && \\rightarrow N(\\mu^*, \\sigma^{*2})\n    \\end{alignat*}\n}\n\\]\n\nThe problem boils down to realizing that we have a summation of two independent Normally distributed random variables, \\(y\\) and \\(u\\). How do independent Normally distributed random variables sum? If you don’t remember, don’t worry, neither did I. After some revision we know that (Ross 2010):\n\\[\n\\displaylines{\n    \\begin{align}\n    \\text{E}[X + Y]   &= \\text{E}[X] + \\text{E}[Y] \\\\\n    \\text{Var}[X + Y] &= \\text{Var}[X] + \\text{Var}[Y] + 2\\text{Cov}[X,Y]\n    \\end{align}\n}\n\\]\nThe assumption that the random variables are independent means the covariance is zero, \\(2\\text{Cov}[X,Y] = 0\\). We can simply add the means and variances (Ross 2010, 256–57) giving:\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    y^* &= y &&+ u \\\\\n        &= \\alpha + \\beta x &&+ \\epsilon + u \\\\\n        &= \\alpha + \\beta x &&+ \\epsilon^* \\\\\n        &= \\alpha + \\beta x &&+ \\Big( N(\\mu_{\\epsilon}, \\sigma_{\\epsilon}^2) + N(\\mu_u, \\sigma_u^2) \\Big) \\\\\n        &= \\alpha + \\beta x &&+ \\Big( N(\\mu_{\\epsilon} + \\mu_u, \\sigma_{\\epsilon}^2 + \\sigma_u^2) \\Big) \\\\\n        &= \\alpha + \\beta x &&+ N(0 + 2, 1 + 4) \\\\\n        &= \\alpha + \\beta x &&+ N(2, 5) \\\\\n    \\end{alignat*}\n}\n\\]\nwhere,\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\mu_{\\epsilon^*} &= \\mu_{\\epsilon} &&+ \\mu_u \\\\\n        &= 0 &&+ 2 \\\\\n        &= 2\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    \\sigma_{\\epsilon^*}^2 &= \\sigma_{\\epsilon}^2 &&+ \\sigma_u^2 \\\\\n        &= 1 &&+ 4 \\\\\n        &= 5\n    \\end{alignat*}\n}\n\\]\n\n\n\n\nPlugging in the values for \\(\\alpha\\) and \\(\\beta\\), we find the following solution: \\[\ny^* = 3 + 2x + N(2, 5)\n\\]\n\n\\(\\alpha^* = 3\\)\n\\(\\beta^* = 2\\)\n\\(\\mu^* = 2\\)\n\\(\\sigma^* = \\sqrt{5}\\)\n\n\n\n\nRemember, we were asked for \\(\\sigma^*\\) not \\(\\sigma^{*2}\\):\nWe can arguably simplify this a bit further by noting that the bias, \\(\\alpha\\), and the mean of the residuals, \\(\\mu_{\\epsilon^*}\\), are both constants that shift the intercept and can be grouped. Subtracting the mean of the error from the bias…\n\\[\n\\displaylines{\n    \\begin{alignat*}{3}\n    y^* &= \\alpha &&+ \\beta x &&&+ N(2, 5) \\\\\n        &= 3      &&+ 2x      &&&+ N(2, 5) \\\\\n        &= (3+2)  &&+ 2x      &&&+ N(0, 5) \\\\\n        &= 5      &&+ 2x      &&&+ N(0,5)\n    \\end{alignat*}\n}\n\\]\nAnd rewritting using the standard deviation rather than the variance (like in the original question) we arrive at:\n\\[\ny = 5 + 2x + N(0,\\sqrt{5})\n\\]\n\n\\(\\alpha^* = 5\\)\n\\(\\beta^* = 2\\)\n\\(\\mu^* = 2\\)\n\\(\\sigma^* = \\sqrt{5}\\)\n\n\n\nIf instead we took 2 as the var in \\(u \\sim N(2,2)\\):\n\n\\(\\alpha^* = 5\\)\n\\(\\beta^* = 2\\)\n\\(\\mu^* = 2\\)\n\\(\\sigma^* = \\sqrt{3}\\)"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#quantative-solution",
    "href": "posts/summing_random_variables/index.html#quantative-solution",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# Quantative solution",
    "text": "# Quantative solution"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#pub-quiz",
    "href": "posts/summing_random_variables/index.html#pub-quiz",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "",
    "text": "Last week, my company held a pub quiz. This was no ordinary pub quiz, it was designed in a way for teams to learn about other teams. One person from each team submitted four questions about their team. We ended up wtih about 60 questions from all across the company. And when I say “all” I mean all. We had questions about Data Science, Commercial, Operations, Engineering, Product, Marketing, Leadership, Human Resources, you name it. As someone who is fairly familiar with all aspects of our products and engineering teams (but by no means an expert in all these areas) by having been here over four years and written my share of documentation, I came into the pub quiz with confidence. Much of that confidence due to expecting to not need to know British pop culture back through the 70s as is standard in an actual pub quiz.\nThat confidence was quickly smashed with questions like:\n\nWhat was the 2023 Coca Cola Christmas commercial slogan?\nWhat is the collective age of the Customer Support team?\nWhat month was repo XXX created?\n\nThe Data Science round was the trickiest of them all, in no small part because reading the question took up almost all of the response time! After the quiz, my teammate who wrote the Data Science questions shared one that didn’t quite make the cut.\nI share the question with the reader here, with only some minor rewording. My answer follows."
  },
  {
    "objectID": "posts/summing_random_variables/index.html#in-the-end-it-didnt-even-matter",
    "href": "posts/summing_random_variables/index.html#in-the-end-it-didnt-even-matter",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# In the end, it didn’t even matter",
    "text": "# In the end, it didn’t even matter\nThat took a bit of work. Thankfully, this question didn’t make it into our company pub quiz. Even if it had, I think everyone (sans the author) would have had to ultimately guess the answer. We only had 30 seconds per question. I didn’t even finish reading the problem statement let alone begin tackling it in that time!\nIn the end, whether this question would have been included or not, wouldn’t have changed the outcome. The quiz was neck and neck between 3 teams for most of the game. My team bouncing between 1\\(^{st}\\) and 3\\(^{rd}\\) place. That is until the final round of questions from the Leadership team, which were highly specific to a certain someone.\n\n\nIf we have another pub quiz, I’ll make sure to sling in some equally maladjusted questions. But I’ll post the answers beforehand here.\n\nWatch this space.\nGood thing I had the CEO on my team 🥇"
  },
  {
    "objectID": "posts/summing_random_variables/index.html#computational-solution",
    "href": "posts/summing_random_variables/index.html#computational-solution",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# Computational solution",
    "text": "# Computational solution\nLet’s check our analytical solution through computational methods. First let’s check that we can indeed simply add the means and variances of independent random variables.\n\n\nCode\nimport torch\n\nn = 1_000\n\nerr = torch.normal(0, 1, size=(n,))\nu = torch.normal(2, 2, size=(n,))\n\nerr_computational = err + u\nerr_analytical = torch.normal(2, 5 ** (0.5), size=(n,))\n\nprint(\n    f\"\"\"\n    Mean: {err_computational.mean()} - {err_analytical.mean()} = {err_computational.mean() - err_analytical.mean()}\n    Var:  {err_computational.var()}  - {err_analytical.var()}  = {err_computational.var()  - err_analytical.var()}\n    \"\"\"\n)\n\n\n\n    Mean: 1.9950406551361084 - 1.9344121217727661 = 0.060628533363342285\n    Var:  5.29583215713501  - 4.611844062805176  = 0.683988094329834\n    \n\n\nThe error between the means and variances is small. If we increase the sample size these errors go closer towards 0.\nNow that we’re confident in random variable arithmetic, let’s check the full analytical solution to a comutational one. Below we plot the first measured sample, \\(y_1\\), with the original signal, \\(y_{\\text{signal}}\\), as a backdrop for reference:\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    y_{\\text{\\scriptsize{signal}}} &= \\alpha &&+ \\beta x \\\\\n                      &= 3      &&+ 2 x\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{3}\n    y_{\\text{1}} &= \\alpha &&+ \\beta x + \\epsilon \\\\\n                 &= 3      &&+ 2 x     + N(\\mu, \\sigma^2) \\\\\n                 &= 3      &&+ 2 x     + N(0, 1)\n    \\end{alignat*}\n}\n\\]\n\n\n\n\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{4}\n    y^*_{\\text{\\scriptsize{analytical}}} &= y      &&+ u \\\\\n        &= \\alpha &&+ \\beta x + \\epsilon^* \\\\\n        &= 1      &&+ 2 x     + N(\\mu^*, \\sigma^{*2}) \\\\\n        &= 5      &&+ 2 x     + N(0, 5) \\\\\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{4}\n    y^*_{\\text{\\scriptsize{computational}}} &= y      &&+ u \\\\\n        &= \\alpha &&+ \\beta x + \\epsilon &&&+ u \\\\\n        &= 3      &&+ 2 x     + N(0, 1)   &&&+ N(2, 4)\n    \\end{alignat*}\n}\n\\]\n\n\n\n\n\n\nCode\nimport altair as alt\nimport pandas as pd\nimport torch\n\nalt.data_transformers.disable_max_rows()\n\nx = torch.arange(0, 10, 0.01)\n\n# Original Signal\na = 3\nb = 2\nsignal = a + (b * x)\n\n# y Our measured sample\nerr_mean = 0\nerr_std = 1\nerr = torch.normal(err_mean, err_std**2, size=(len(x),))\ny = a + (b * x) + err\n\n# y* Analytical\na = 5\nb = 2\nerr_mean = 0\nerr_std = 5 ** (0.5)\nerr_analytical = torch.normal(err_mean, err_std, size=(len(x),))\ny_analytical = a + (b * x) + err_analytical\n\n# y* Computational\na = 3\nb = 2\nerr_mean = 2\nerr_std = 5 ** (0.5)\nerr_computational = torch.normal(err_mean, err_std, size=(len(x),))\ny_computational = a + (b * x) + (err + err_computational)\n\n# Data munging\ndf0 = pd.DataFrame({\"x\": x, \"y\": signal, \"dataset\": [\"signal (original)\"] * len(x)})\ndf1 = pd.DataFrame({\"x\": x, \"y\": y, \"dataset\": [\"y\"] * len(x)})\ndf2 = pd.DataFrame(\n    {\"x\": x, \"y\": y_computational, \"dataset\": [\"y* (computational)\"] * len(x)}\n)\ndf3 = pd.DataFrame({\"x\": x, \"y\": y_analytical, \"dataset\": [\"y* (analytical)\"] * len(x)})\n\ndf = pd.concat([df0, df1, df2, df3])\n\n# Plot\ncolorscheme = [\n    \"#368BC1\",  # blue\n    \"#F2BB18\",  # yellow\n    \"#BB4430\",  # red\n    \"#8A9A67\",  # green\n    \"#CC771F\",  # orange\n    \"#8B5260\",  # purple\n]\nselection = alt.selection_point(fields=[\"dataset\"], bind=\"legend\")\nchart = (\n    alt.Chart(\n        df,\n        title=\"Computational simulation of the problem matches the analytical solution\",\n    )\n    .mark_circle(size=10)\n    .encode(\n        x=\"x:Q\",\n        y=\"y:Q\",\n        color=alt.Color(\n            \"dataset:N\", title=None, scale=alt.Scale(range=colorscheme)\n        ).legend(orient=\"top-left\"),\n        opacity=alt.condition(selection, alt.value(0.75), alt.value(0.2)),\n    )\n    .configure(background=\"#f8f9fa\")\n    .properties(width=650)\n    .add_params(selection)\n)  # .interactive()\n\nchart.display()\n\neps_str = err + err_computational\nstats_df = pd.DataFrame(\n    {\n        \"mean\": [\n            err.mean().item(),\n            eps_str.mean().item(),\n            err_analytical.mean().item(),\n        ],\n        \"var\": [\n            err.var().item(),\n            eps_str.var().item(),\n            err_analytical.var().item(),\n        ],\n    },\n    index=[\"original\", \"computational\", \"analytical\"],\n)\n\nprint(stats_df)\n\n\n\n\n\n\n\n\n                   mean       var\noriginal       0.030476  0.973765\ncomputational  2.039693  5.971036\nanalytical    -0.110362  4.906000\n\n\n\n\nNote, torch.normal takes the std dev instead of the variance.\n\nTo see each series more clearly, click on one in the legend.\n\nThe chart and table show both the computational analysis matches the analytical analysis. The table also reflects these findings."
  },
  {
    "objectID": "posts/summing_random_variables/index.html#computational-verification",
    "href": "posts/summing_random_variables/index.html#computational-verification",
    "title": "Pub Quiz: Summing Two Random Variables",
    "section": "# Computational verification",
    "text": "# Computational verification\nLet’s check our analytical solution through computational methods. First let’s check that we can indeed simply add the means and variances of independent random variables.\n\n\nCode\nimport torch\n\nn = 1_000\n\nerr = torch.normal(0, 1, size=(n,))\nu = torch.normal(2, 2, size=(n,))\n\nerr_computational = err + u\nerr_analytical = torch.normal(2, 5 ** (0.5), size=(n,))\n\nprint(\n    f\"\"\"\n    mean: {err_computational.mean()} - {err_analytical.mean()} = {err_computational.mean() - err_analytical.mean()}\n    var:  {err_computational.var()}  - {err_analytical.var()}  = {err_computational.var()  - err_analytical.var()}\n    \"\"\"\n)\n\n\n\n    mean: 1.9502878189086914 - 2.0952625274658203 = -0.1449747085571289\n    var:  4.974301815032959  - 4.614882946014404  = 0.3594188690185547\n    \n\n\nThe error between the means and variances is small. If we increase the sample size these errors go closer towards 0.\nNow that we’re confident in random variable arithmetic, let’s check the full analytical solution to a comutational one. Below we plot the first measured sample, \\(y_1\\), with the original signal, \\(y_{\\text{signal}}\\), as a backdrop for reference:\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{2}\n    y_{\\text{\\scriptsize{signal}}} &= \\alpha &&+ \\beta x \\\\\n                      &= 3      &&+ 2 x\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{3}\n    y_{\\text{1}} &= \\alpha &&+ \\beta x + \\epsilon \\\\\n                 &= 3      &&+ 2 x     + N(\\mu, \\sigma^2) \\\\\n                 &= 3      &&+ 2 x     + N(0, 1)\n    \\end{alignat*}\n}\n\\]\n\n\n\n\n\n\n\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{4}\n    y^*_{\\text{\\scriptsize{analytical}}} &= y      &&+ u \\\\\n        &= \\alpha &&+ \\beta x + \\epsilon^* \\\\\n        &= 1      &&+ 2 x     + N(\\mu^*, \\sigma^{*2}) \\\\\n        &= 5      &&+ 2 x     + N(0, 5) \\\\\n    \\end{alignat*}\n}\n\\]\n\n\\[\n\\displaylines{\n    \\begin{alignat*}{4}\n    y^*_{\\text{\\scriptsize{computational}}} &= y      &&+ u \\\\\n        &= \\alpha &&+ \\beta x + \\epsilon &&&+ u \\\\\n        &= 3      &&+ 2 x     + N(0, 1)   &&&+ N(2, 4)\n    \\end{alignat*}\n}\n\\]\n\n\n\n\n\n\nCode\nimport altair as alt\nimport pandas as pd\nimport torch\n\nalt.data_transformers.disable_max_rows()\n\nx = torch.arange(0, 10, 0.01)\n\n# Original Signal\na = 3\nb = 2\nsignal = a + (b * x)\n\n# y Our measured sample\nerr_mean = 0\nerr_std = 1\nerr = torch.normal(err_mean, err_std**2, size=(len(x),))\ny = a + (b * x) + err\n\n# y* Analytical\na = 5\nb = 2\nerr_mean = 0\nerr_std = 5 ** (0.5)\nerr_analytical = torch.normal(err_mean, err_std, size=(len(x),))\ny_analytical = a + (b * x) + err_analytical\n\n# y* Computational\na = 3\nb = 2\nerr_mean = 2\nerr_std = 5 ** (0.5)\nerr_computational = torch.normal(err_mean, err_std, size=(len(x),))\ny_computational = a + (b * x) + (err + err_computational)\n\n# Data munging\ndf0 = pd.DataFrame({\"x\": x, \"y\": signal, \"dataset\": [\"signal (original)\"] * len(x)})\ndf1 = pd.DataFrame({\"x\": x, \"y\": y, \"dataset\": [\"y\"] * len(x)})\ndf2 = pd.DataFrame(\n    {\"x\": x, \"y\": y_computational, \"dataset\": [\"y* (computational)\"] * len(x)}\n)\ndf3 = pd.DataFrame({\"x\": x, \"y\": y_analytical, \"dataset\": [\"y* (analytical)\"] * len(x)})\n\ndf = pd.concat([df0, df1, df2, df3])\n\n# Plot\ncolorscheme = [\n    \"#368BC1\",  # blue\n    \"#F2BB18\",  # yellow\n    \"#BB4430\",  # red\n    \"#8A9A67\",  # green\n    \"#CC771F\",  # orange\n    \"#8B5260\",  # purple\n]\nselection = alt.selection_point(fields=[\"dataset\"], bind=\"legend\")\nchart = (\n    alt.Chart(\n        df,\n        title=\"Computational simulation of the problem matches the analytical solution\",\n    )\n    .mark_circle(size=10)\n    .encode(\n        x=\"x:Q\",\n        y=\"y:Q\",\n        color=alt.Color(\n            \"dataset:N\", title=None, scale=alt.Scale(range=colorscheme)\n        ).legend(orient=\"top-left\"),\n        opacity=alt.condition(selection, alt.value(0.75), alt.value(0.2)),\n    )\n    .configure(background=\"#f8f9fa\")\n    .properties(width=650, padding=10)\n    .add_params(selection)\n)  # .interactive()\n\nchart.display()\n\nstats_df = pd.DataFrame(\n    {\n        \"mean\": [\n            err.mean().item(),\n            err_computational.mean().item(),\n            err_analytical.mean().item()\n            + 2,  # un grouping biases to reflect the contribution of the mean of residuals to the bias\n        ],\n        \"var\": [\n            err.var().item(),\n            err_computational.var().item(),\n            err_analytical.var().item(),\n        ],\n    },\n    index=[\"y (original)\", \"y* (computational)\", \"y* (analytical)\"],\n)\n\nprint(stats_df)\n\n\n\n\n\n\n\n\n                        mean       var\ny (original)       -0.011793  1.007046\ny* (computational)  2.021486  5.338953\ny* (analytical)     2.090091  5.163354\n\n\n\n\nNote, torch.normal takes the std dev instead of the variance.\n\nTo see each series more clearly, click on one in the legend.\n\nThe chart shows both the computational analysis verifies the analytical solution while the table above shows the mean and variance of the residuals of the computational anlysis is the same as the analytical solution. The table of statistics for each series also reflects these findings."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "The Bear's Toes",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 Robert Edwards\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  }
]